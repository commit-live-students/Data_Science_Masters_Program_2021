{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "NLP or Natural Language Processing as is normally referred to, refers to working (or processing) **text data**, either for machine learning, or any of the host of usecases textual data comprises of. Working with text, is very different from working with numerical or categorical data. We have worked extensively with data, numerical, categorical and boolean, however text data is a different paradigm altogether and this tutorial aims to get you acquanited with the basics of working with text and understanding the underlying implications in Machine learning.  \n",
    "\n",
    "## Overview\n",
    "\n",
    "- Introduction to the problem statement **Consumer Complaints Database**\n",
    "- What is NLP (Introduction and usecases)\n",
    "- Tokenization and Introduction to NLTK\n",
    "- Vectorization and vector space models **Count Vectorizer**\n",
    "- Applying our first classification algorithm **Logistic Regression**\n",
    "- Stopwords\n",
    "- Basic Stemming \n",
    "- TFIDF\n",
    "- Naive Bayes Classifier\n",
    "- Linear kernel SVM\n",
    "- Text Classification (Build a text classifier using NLTK)\n",
    "\n",
    "\n",
    "## Pre-requisite\n",
    "\n",
    "- Python (along with NumPy and pandas libraries)\n",
    "- Basic statistics (knowledge of central tendancy)\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "- Understanding why working with text data isn't like numerical or categorical data\n",
    "- What is NLP\n",
    "- The basic building blocks of text\n",
    "- Tokenization, Stemming and what constitutes as a stopword\n",
    "- Preliminary cleaning of text data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Introduction to text data\n",
    "\n",
    "### Description: \n",
    "Until now, all of our problem statements had data in either a numerical format, a categorical format, or a Boolean format. In the real-world we usually do, and might very well encounter text data. We will now try to understand how we can use text analytics to solve data with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction to the problem statement: <font color='green'> Categorize complaints into categories</font>\n",
    "\n",
    "**What is the problem?**\n",
    "\n",
    "Let us get started with the introduction to natural language processing by first looking at the problem that we are going to solve. We have a rich dataset of consumer complaints on various financial products and services. Each row in the dataset describes the complaint and the different features associated with it. In this concept, we'll first construct the features and then build a model that predicts the category into which the complaint falls. You can read more about this dataset [here](https://catalog.data.gov/dataset/consumer-complaint-database).\n",
    "\n",
    "Along with the complaint narrative, the other features that are present in data are the issue, the category of the complaint, the date it was received on, the zip code, details of the customer placing the complaint and the current status of the complaint. The final idea is to build a model that will categorize each customer's complaint into a product (12 categories in all). \n",
    "\n",
    "For the purpose of understanding how text processing works, we will specifically, work on only 2 columns of this dataset. It is evident that if we add more features, the model accuracy will rise and be more robust. \n",
    "\n",
    "**Brief explanation of the dataset & features**\n",
    "\n",
    "- `Consumer Complaint Narrative`: This is a paragraph (or text) written by the customer explaining his complaint in detail. The data is a string type consisting of text in the form of paragraphs.\n",
    "- `Product`: This is the category we are to classify each complaint to. The 12 categories the complaints need to be categorized into are: \n",
    "\n",
    "'Mortgage', 'Student loan', 'Credit card or prepaid card', 'Credit card', 'Debt collection', 'Credit reporting', 'Credit reporting, credit repair services, or other personal consumer reports', 'Bank account or service', 'Consumer Loan', 'Money transfers', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Checking or savings account', 'Payday loan', 'Payday loan, title loan, or personal loan', 'Other financial service', 'Prepaid card'\n",
    "\n",
    " \n",
    "**What we want as the outcome?**\n",
    "\n",
    "We would classify each complaint to its respective category, so that the complaint can be directed to the right vertical.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "In this task you will load Consumer_complaints.csv into a dataframe using pandas and explore the column Consumer Complaint Narrative.\n",
    "\n",
    "- Load the dataset from `'path'`(given) using the `read_csv()` method from pandas and store it in `'full_data'`. \n",
    "\n",
    "- Subset the dataframe  `'full_data'` to only include `\"Consumer complaint narrative\"` and  `\"Product\"` and store this dataframe subset in `'data'`\n",
    "\n",
    "- Rename the column `\"Consumer complaint narrative\"` to `\"X\"` and `\"Product\"` to `\"y\"` by assigning `[\"X\",\"y\"]` to `data.columns` \n",
    "\n",
    "(The reason we have done this is obvious. We intend to classify the complaints in the narrative (X) to each of the categories listed in the Product (or the Y column))\n",
    "\n",
    "- Print out the first value of the `\"X\"` column to take a look at it.\n",
    "\n",
    "- Also print column `'y'` and have a look at the various categories that exist in the Product (the renamed y column)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:54.963196Z",
     "start_time": "2018-12-24T17:44:54.941390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "\n",
      "['Mortgage', 'Student loan', 'Credit card or prepaid card', 'Credit card', 'Debt collection', 'Credit reporting', 'Credit reporting, credit repair services, or other personal consumer reports', 'Bank account or service', 'Consumer Loan', 'Money transfers', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Checking or savings account', 'Payday loan', 'Payday loan, title loan, or personal loan', 'Other financial service', 'Prepaid card']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df_copy = df.copy()\n",
    "\n",
    "path = \"../data/new_complaints.csv\"\n",
    "#path = \"/Users/greyatom/Desktop/complaints.csv\"\n",
    "# Loading of dataset\n",
    "full_data = pd.read_csv(path)\n",
    "\n",
    "# keeping the relevant columns\n",
    "data = full_data[[\"Consumer complaint narrative\", \"Product\"]]\n",
    "data.columns = [\"X\", \"y\"]\n",
    "data.head()\n",
    "\n",
    "# Printing out the first non-empty value of the X column. Hence the second value, index is 1\n",
    "print(data[\"X\"][1])\n",
    "print(\"\\n\")\n",
    "print(list(data[\"y\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Motivation and Uses of NLP\n",
    "\n",
    "\n",
    "At the end of the previous exercise we saw that the data contained in the consumer complaint narrative column is a **paragraph**.  \n",
    "\n",
    "This is a typical example of text data - words combine to form sentences, and sentences combine to form paragraphs. The column `Consumer complaint narrative` has all rows in the form of either NaNs or text data. How do we make sense of this data?\n",
    "Do we convert this to categorical format through one-hot encoding? If yes, how do we do it?\n",
    "How do we convert the text data to a numerical format to make sure machine learning algorithms can be applied?\n",
    "Can this column be used in a multinomial classification model to predict the class of the complaint?\n",
    "\n",
    "All these questions (and more) can be answered through a branch of Artificial Intelligence. Enter **Natural Language Processing.**\n",
    "\n",
    "\n",
    "Formally, Natural Language Processing or NLP is defined as the application of computational techniques for the analysis and the synthesis of text. The aim of NLP is to give computers the ability to do tasks involving human language. In terms of hands-on or engineering terms, it can broadly be defined as \"cleaning\" and \"transforming\" text to a form fit for machine learning. Of course, you can derive insights from text as well just like any EDA operation. But the inherent properties of text call for a principally different approach to deal with it. This leads us to an important question of why is to so hard to deal with text that it requires a separate field of study.\n",
    "\n",
    "\n",
    "\n",
    "<!--There are other off shoots of Natural Langugage such as NLG - Natural language Generation which aims to generate new text data based on prior data and Natural Language Understanding - NLU which is the backbone of all intelligent chatbots out there currently, which focuses on recognizing the intent of a conversation. For the sake of this tutorial and brevity we will stick to NLP. -->\n",
    "\n",
    "\n",
    "\n",
    "#### Why is it difficult to work with text?\n",
    "\n",
    "***\n",
    "\n",
    "Comprehending Language is hard for computers. Some of the unique challenges of working with text are as follows: \n",
    "\n",
    "- **Synonymy** - This corresponds to different words having the same meaning. A similar intent can be conveyed in various ways and this is one of the prime reasons, why computers have a hard time deciphering the meaning or intent of those statements. \"The President of United States has signed a new decree\" and \"POTUS has inked in a new law\" are basically advocating the same sentiment. However as they are completely different sentences syntactically, computers have a hard time figuring out the user intent.\n",
    "\n",
    "\n",
    "- **Ambiguity** - \"The bank deposit rate is quite high\" and \"He stood near the bank admiring the river\". In these statements, the word `bank` has completely different meanings. In the first case it represents a financial institution, and in the second case it refers to land near the river. Disambiguating the meaning in sentences is quite challenging.  \n",
    "\n",
    "\n",
    "- **Anaphora Resolution** - \"George is my friend. He likes football\". In the second statement `he` refers to George. It is difficult for the computers to discern what person/entity the pronoun `he` is referring to.  \n",
    "\n",
    "\n",
    "- **Language related issues** - Every language has its own uniqueness. For English we have words, sentences, paragraphs and so on. But in Thai, there is no concept of sentences at all! The grammar and morphology of languages is so different. This is why we observe that Google Translator or any other translator service struggles to perfectly convert a piece of text from one language to another.\n",
    "\n",
    "\n",
    "- **Out of Vocabulary problem** - Machines have a hard time adapting to any new constructs that humans come up with. As humans when we come across a word we haven't seen earlier, we might not understand its meaning instantly. But this does not mean we cannot adapt. After looking at the word in several different sentences and understanding its usage, we understand the context and meaning of the new word. Machines can only handle data that they have seen before. It is unable to adapt well.\n",
    "\n",
    "\n",
    "- **Language generation** - While language understanding is hard, language generation too has its own set of challenges. For chatbots to work effectively, they need to communication properly constructed sentences which are grammatically correct. This is quite a hard problem and a challenge that needs to be overcome. \n",
    "\n",
    "\n",
    "We now know that working with text is hard. But there are also exciting applications and use cases involved with working on text. We will now take a look at some of the use cases. \n",
    "\n",
    "#### Usecases of NLP\n",
    "\n",
    "***\n",
    "\n",
    "The usecases of NLP encompass almost anything you can do with Language in relation to a problem. \n",
    "\n",
    "1) **Sentiment Analysis** - Finding if the text is leaning towards a positive or negative sentiment.\n",
    "\n",
    "The process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral is called Sentiment Analysis. The information present over the Internet is constantly growing resulting in a large number of texts expressing opinions in review sites, forums, blogs and different social media forums. Sentiment analysis is therefore a topic of great interest and development since it has many practical applications. It is immensely useful in figuring the overall sentiment of products (Amazon), movies (Netflix), food (Yelp),etc. Its applications include Market Research, Social Monitoring, Customer Support and Product Analytics.\n",
    "\n",
    "\n",
    "2) **Text Classification** - Categorizing text to various categories\n",
    "\n",
    "\n",
    "Text classifiers can be used to organize, structure, and categorize almost any text data we have. For e.g. New articles can be organized by topics, chat conversations can be organized by language, support tickets can be organized by urgency etc. Other examples of text classification include:\n",
    "\n",
    "- Directing customer queries to the right vertical\n",
    "\n",
    "- Detection of spam and non-spam emails,\n",
    "\n",
    "- Auto tagging of customer queries\n",
    "\n",
    "\n",
    "3) **Document Summarization** - Compressing a paragraph/document into few words or sentences\n",
    "\n",
    "Text summarization is the method of compressing a text document, in order to create a summary of the major points of the document. The idea of summarization is to find a subset of data which contains the `information` of the entire set. It's applications include News summary(Inshorts app), Novel Summary, Book Summary (Blinkist) etc. With the overall attention span declining, the need to provide information in the shortest possible words has risen - and summarization helps solve this problem.\n",
    "\n",
    "4) **Parts of Speech Tagging** - Figuring out the various nouns, adverbs, verbs etc in the text\n",
    "\n",
    "Identifying part of speech tags is much more complicated than it looks. This is because over time in the development of language, a single word can have different parts of speech tag in different sentences based on different contexts. This makes it impossible to have a generic mapping for POS tags. Few of its applications include:\n",
    "\n",
    "- Text to speech conversion\n",
    "\n",
    "- Word Sense Disambiguation (Teach machine to know the difference of the meaning of word 'bears' in \"I saw a couple of bears\" and \"Hard work always bears fruit\")\n",
    "\n",
    "\n",
    "5) **Machine translation** - Translate text from one language to another\n",
    "\n",
    "Machine Translation is the task of automatically translating one natural language into another while retaining the meaning of the original text. Translation from one language to another is complex because some of the words in the original language could have multiple meanings and these words could have different forms in the output language. \n",
    "Its most popular application is Google Translate and it is employed in devices like Google Home as well. Machine translation allows business transactions between partners in different countries without the need of a human interpreter. \n",
    "\n",
    "6) **Named Entity Recognition** - Identify the entities present in text\n",
    "\n",
    "Named Entity Recognition deals with named entity mentions in text and categorizes these entities into person, organization, datetime reference etc. This is used a lot in the field of bioinformatics, molecular biology and other medical NLP applications. It also plays an important role in the overall field of Information Extraction where we try to extract knowledge from unstructured text. \n",
    "\n",
    "7) **Conversational AI** - Chat with a machine in natural language and get queries resolved\n",
    "\n",
    "Conversational AI deals with creating an interface between machines and humans to converse in natural language. Such interfaces are known as chatbots. A user can interact in natural language with natural language, the same way he usually communicates with a human. For organizations to truly scale in terms of customer support, chatbots are increasingly adopted as the first point of contact for customer query resolution across all organizations.\n",
    "\n",
    "\n",
    "So for enabling all the NLP usecases, the first challenge is to convert the text into a form that the machine can understand. For that, we need to arrive at a fundamental component of text known as `tokens`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Tokenization \n",
    "\n",
    "\n",
    "### Motivation for tokenization\n",
    "\n",
    "We can see that unlike all the machine learning datasets we have worked with previously, the data isn't boolean, numeric, categorical etc. Usually a text is composed of paragraphs, paragraphs are composed of sentences, and sentences are composed of words. You could also go deeper into letters, but the letters have no meaning. It's only when they are combined into words, that the text starts to make sense. Hence, it is better to work at the word level. \n",
    "\n",
    "Tokenization is the process of splitting the text into smaller parts called tokens. Tokens are the basic units of a particular dataset. The choice of tokens could be based on the application we are working on. \n",
    "\n",
    "#### Introduction to NLTK\n",
    "\n",
    "\n",
    "Natural Language Tool Kit/NLTK is the standard library in python which specifically deals with text. All the text processing tasks could be easily done with this library. It is a leading platform for building Python programs to work with human language data. It also provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, along with an active discussion forum. On top of it, it is completely free and open-source with a vibrant developer community supporting it. Let us now take the first step towards categorizing the consumer complaints by starting with tokenization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Tokenizing with NLTK - The problem intuition \n",
    "\n",
    "***\n",
    "\n",
    "We will first need to find a way to convert the text to numbers to get them to a form where you would be able to apply an algorithm to this. Think of this like sklearn, which require all non-numeric data to be encoded (label or one-hot) prior to the sklearn pipeline. \n",
    "\n",
    "Intuitively, it would make sense to divide each paragraph of text to its basic form (words) and then convert each of those words to numbers. We could assign a particular number to each word, in which case a sentence could look like a set of numbers to us, each number representing a particular word. \n",
    "\n",
    "The first step to achieving that would be to break the text down to words. That's what tokenization aims to do. NLTK has a built in libraries for tokenization which we will use for our purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the first complaint into words\n",
    "\n",
    "In this task you will assign a variable to the first row of the consumer complaints narrative column (X column) and break down the text into it's constituent words.\n",
    "### Instructions\n",
    "\n",
    "- Drop nan values from the entire dataframe `data` using `\"dropna()\"` using `inplace=True`\n",
    "\n",
    "\n",
    "- Save the first value of column `\"X\"`(consumer complaint narrative) in a variable called `'first_complaint'` \n",
    "\n",
    "\n",
    "- Break down `'first_complaint'` into words using `\"split()\"` function and store the result in a variable called `'bag_of_words_1'` \n",
    "\n",
    "\n",
    "- Break down `'first_complaint'` into tokens using `\"word_tokenize()\"` method(i.e. `\"word_tokenize(first_complaint)\"`) and store the result in a variable called `'bag_of_words_2'` \n",
    "\n",
    "\n",
    "- Compare both bag of words.\n",
    "\n",
    "\n",
    "**Observation:**\n",
    "\n",
    "We can see from both lists (the one using split and the one using word_tokenize) that the word_tokenize function is more robust as it splits the paragraph into purely words and seperates the punctuation into seperate tokens. In the case of split, full-stops have appeared along with certain words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.008051Z",
     "start_time": "2018-12-24T17:44:54.965126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Complaint\n",
      "\n",
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Using the Split Command\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up.']\n",
      "\n",
      "Using tokenize\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Dropping nan values from dataframe\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Storing the first complaint\n",
    "first_complaint = data.iloc[0][0]\n",
    "\n",
    "\n",
    "# Printing the first complaint\n",
    "print(\"\\nFirst Complaint\\n\")\n",
    "print(first_complaint)\n",
    "\n",
    "# Using the split command\n",
    "print(\"\\nUsing the Split Command\\n\")\n",
    "bag_of_words_1 = first_complaint.split(\" \")\n",
    "print(bag_of_words_1)\n",
    "\n",
    "# Using the tokenize command\n",
    "print(\"\\nUsing tokenize\\n\")\n",
    "bag_of_words_2 = word_tokenize(first_complaint)\n",
    "print(bag_of_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Sent Tokenize\n",
    "\n",
    "\n",
    "One could also tokenize a paragraph into constituent sentences. \n",
    "\n",
    "To split into definite sentences, it is compulsory to identify the end and beginning of sentences. In case if sentences ending with **!** and  **?**, it is quite clear that they signal the end of a sentence; and consequently the next word is the start of a new sentence. But **periods (.)** are ambiguous. This is because periods can be used for:\n",
    "- Sentence boundary\n",
    "- Abbreviations (ex: Dr. Mr. etc.)\n",
    "- Numbers (0.24, -0.43)\n",
    "\n",
    "So how to identify periods which actually signify sentence boundaries? Some of the common ways to address this issue are:\n",
    "- Use handwritten rules. For ex: If there is a word starting with \"An\" after a period then it is a sentence boundary\n",
    "- A classical way is to use regular expressions \n",
    "- Modern methods include usage of machine learning models. For ex: Sequence modelling is a very popular choice for this use case.\n",
    "\n",
    "The importance of converting words to lower case - All words should be converted to lowercase while doing NLP. The reason behind being, that \"Yorkshire\" and \"yorkshire\" even though are the same word, will be considered 2 separate words while converting the words into numbers. Since both of them represent the same word, it would lead to redundant features. Most of the classifiers assume that the features are independent of each other. To avoid such issues, it is standard practice to convert all words or text to lower case, before beginning NLP.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ### Tokenize the first complaint into  sentences\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Break down `'first_complaint'` into sentences using the `\"sent_tokenize()\"` method(i.e. `\"sent_tokenize(first_complaint)\"`) from nltk and  to another list called list_of_sentences\n",
    "\n",
    "\n",
    "- Convert `first_complaint` into lowercase using `\"lower()\"` method and store it in a variable called `'first_complaint_lower'`.\n",
    "\n",
    "- Break down `'first_complaint_lower'` into tokens using `\"word_tokenize()\"` method and store the result in a variable called `'bag_of_words_lower'` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.014823Z",
     "start_time": "2018-12-24T17:44:55.009726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sentences\n",
      " ['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not.', 'When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX.', 'I have been faithful at paying my student loan.', 'I was told that Navient was the company i had delinquency with.', 'I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me.', 'I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus.', 'I have had so much trouble bringing my credit score back up.']\n",
      "\n",
      " ['when', 'my', 'loan', 'was', 'switched', 'over', 'to', 'navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'xxxx', 'i', 'did', 'not', '.', 'when', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'xxxx', 'into', 'the', 'xxxx', '.', 'i', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'i', 'was', 'told', 'that', 'navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'i', 'contacted', 'navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'i', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'i', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "# first_complaint is already loaded onto the workspace\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sentence tokenizing\n",
    "list_of_sentences = sent_tokenize(first_complaint)\n",
    "\n",
    "# Print list of sentences\n",
    "print(\"List of sentences\\n\", list_of_sentences)\n",
    "\n",
    "# Lowering first complaint\n",
    "first_complaint_lower = first_complaint.lower()\n",
    "\n",
    "# Tokenizing first complaint lower\n",
    "bag_of_words_lower = word_tokenize(first_complaint_lower)\n",
    "\n",
    "\n",
    "print(\"\\n\",bag_of_words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Stemming and Lemmatization \n",
    "\n",
    "\n",
    "Owing to grammatical reasons, documents are going to use different forms of a word, such as discuss, discusses and discussing. Along with there are families of derivationally related words with similar meanings, such as liberal, liberty, and liberalization.\n",
    "\n",
    "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of NLP  that are used to prepare text, words, and documents for further processing.\n",
    "\n",
    "The goal of both the methods(stemming and lemmatization) is to reduce inflectional forms and derivationally related forms of a word to a common base form. \n",
    "\n",
    "For eg:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be \n",
    "\n",
    "lion, lions, lion's, lions' $\\Rightarrow$ lion\n",
    "\n",
    "Let's look at them one by one.\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Stemming is the process of converting the words of a sentence to its non-changing portions. \n",
    "So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n",
    "\n",
    "For eg: Likes, liked, likely, unlike $\\Rightarrow$ like\n",
    "\n",
    "Lot of different algorithms have been defined for the process, each with their own set of rules. The popular ones include:\n",
    "\n",
    "\n",
    "* Porter Stemmer(Implemented in almost all languages)\n",
    "\n",
    "* Paice Stemmer\n",
    "\n",
    "* Lovins Stemmer\n",
    "\n",
    "We won't be getting into their details. Feel free to explore their exact mechanisms.\n",
    "\n",
    "Let's see its python implementation:\n",
    "\n",
    "```python\n",
    "\n",
    "import nltk\n",
    "\n",
    "text=\"Natural Language Processing is really fun and I want to study it more\"\n",
    "print(\"The words of text:\",text,\"\\nis stemmed in the following way: \")\n",
    "\n",
    "#Breaking the sentence to words\n",
    "tokens=text.split()\n",
    "\n",
    "#Defining Porter Stemmer object\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "#Applying the stemming\n",
    "stem = [porter.stem(i) for i in tokens]\n",
    "print(stem)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "\n",
    "\"The words of text: Natural Language Processing is really fun and I want to study it more is stemmed in the following way:\" \n",
    "\n",
    "['natur', 'languag', 'process', 'is', 'realli', 'fun', 'and', 'I', 'want', 'to', 'studi', 'it', 'more']\n",
    "\n",
    "```\n",
    "\n",
    "**Lemmatization:**\n",
    "\n",
    "This method is a more refined way of breaking words through the use of a vocabulary and morphological analysis of words. The aim is to always return the base form of a word known as `lemma`.\n",
    "\n",
    "Consider the following words:\n",
    "\n",
    "'Studied', 'Studious' ,'Studying'\n",
    "\n",
    "Stemming of them will result in `Studi`\n",
    "\n",
    "\n",
    "Lemmatisation of them will result in `Study`\n",
    "\n",
    "As it can be seen Lemmatization is more complex than stemming because it requires words to be categorized by a part-of-speech as well as by inflected form. \n",
    "\n",
    "In languages other than English, it can become quite complicated.\n",
    "\n",
    "Let's see its python implementation:\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "text = \"Women in  technology are amazing at coding\"\n",
    "print(\"The words of text:\",text,\"\\nis lemmatized in the following way: \")\n",
    "\n",
    "tokens=text.lower().split()\n",
    "lemma = WordNetLemmatizer()\n",
    "lemma_result = [lemma.lemmatize(i) for i in tokens]\n",
    "print(lemma_result)\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "\"The words of text: Women in  technology are amazing at coding  is stemmed in the following way:\" \n",
    "['woman', 'in', 'technology', 'are', 'amazing', 'at', 'coding']\n",
    "\n",
    "```\n",
    "\n",
    "Compare that with stemming of the same sentence:\n",
    "\n",
    "```python\n",
    "\n",
    "\"The words of text: Women in  technology are amazing at coding is stemmed in the following way:\" \n",
    "['women', 'in', 'technolog', 'are', 'amaz', 'at', 'code']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chapter 2: Vectorization\n",
    "\n",
    "Description: In this chapter, we will talk about how to convert text into numeric form that can be an input to a machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Vectorization\n",
    "\n",
    "\n",
    "Till now, we have arrived at the constituent elements of text. Now the next question to be answered is -  How do we convert this text data to a form fit for machine learning? The usual ways of working with numerical or categorical data will not work here, as the data type is completely different, and the algorithm has to make sense of the written data. \n",
    "\n",
    "\n",
    "**Bag of words:**\n",
    "\n",
    "The problem with modeling text is that there is no well defined fixed-length inputs.\n",
    "\n",
    "A bag of words model is a way of extracting features from text for use in modeling. In this approach, we use the tokenized words for each observation and find out the frequency of each token.\n",
    "\n",
    "Let's take an example to understand it.\n",
    "\n",
    "Consider the following sentences:\n",
    "\n",
    "\"Hope is a good thing\" \n",
    "\"Maybe the best thing\" \n",
    "\"No good thing ever dies\"\n",
    "\n",
    "We will treat each sentence as a different document and make a list of all unique words from the three documentations. We get:\n",
    "\n",
    "`\"hope\", \"is\", \"a\", \"good\", \"thing\", \"maybe\", \"the\", \"best\", \"no\", \"ever\", \"dies\"`\n",
    "\n",
    "Next, we try to create vectors from it.\n",
    "\n",
    "\n",
    "In this, we take the first document = \"Hope is a good thing\" and check the frequency of words from the 10 unique words:\n",
    "\n",
    "\"hope\" - 1\n",
    "\"is\" - 1\n",
    "\"a\" - 1\n",
    "\"good\" - 1\n",
    "\"thing\" - 1\n",
    "\"maybe\" - 0\n",
    "\"the\"-0\n",
    "\"best\" - 0\n",
    "\"no\" - 0\n",
    "\"ever\" - 0\n",
    "\"dies\" - 0\n",
    "\n",
    "Following is how each document will look like:\n",
    "\n",
    "\"Hope is a good thing\"  - [1,1,1,1,1,0,0,0,0,0,0]\n",
    "\n",
    "\"Maybe the best thing\" - [0,0,0,0,1,1,1,1,0,0]\n",
    "\n",
    "\"No good thing ever dies\" - [0,0,0,1,1,0,0,0,0,1,1]\n",
    "\n",
    "\n",
    "**This process of converting text data to numbers is called vectorization**\n",
    "\n",
    "\n",
    "There are multiple methods to convert words to numbers. \n",
    "\n",
    "We will be start with discussing the count Vectorizer. \n",
    "\n",
    "### The count vectorizer\n",
    "\n",
    "***\n",
    "\n",
    " Count Vectorizer works on term frequency and building a sparse matrix of documents x tokens.\n",
    "\n",
    "For e.g. In the dataset in which we are working, the X column is a list of lowercased words \n",
    "\n",
    "The idea now is to convert the X column to numbers. \n",
    "\n",
    "**One way to do that would be to represent every word as a key value pair in the form of a dictionary, where the key would be the word and the value would be the number of times that word has appeared in the list.** \n",
    "\n",
    "![](../images/img2.png)\n",
    "\n",
    "This method of converting the counts of words in the list to convert them to a numeric format is called Count vectorization. \n",
    "\n",
    "We will initially do this manually, and then exploit sklearn to do this automatically to understand the intuition behind this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convert the first complaint to numbers using the counts of words in the form of a dictionary\n",
    "\n",
    "In this task you will implement your own code for count vectorization\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- We have already created `'bag_of_words_lower'`.\n",
    "\n",
    "\n",
    "- Let's try to create a dictionary so that the keys are the words themselves and the values are the number of times the word has appeared in the list `'bag_of_words_lower'`.\n",
    "\n",
    "\n",
    "- Pass `'bag_of_words_lower'` to the `\"Counter()\"` method and store the result in a variable called `'count_vectorizer'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.021184Z",
     "start_time": "2018-12-24T17:44:55.017821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 11, 'the': 8, '.': 7, 'to': 5, 'was': 5, 'and': 5, 'credit': 4, 'had': 4, 'my': 4, 'with': 3, 'that': 3, 'told': 3, 'xxxx': 3, 'navient': 3, 'balance': 2, 'score': 2, 'when': 2, 'delinquency': 2, 'been': 2, 'a': 2, 'bureaus': 2, 'have': 2, 'just': 2, 'loan': 2, 'so': 2, 'kept': 1, 'bringing': 1, 'because': 1, 'never': 1, 'then': 1, 'from': 1, 'company': 1, 'did': 1, 'they': 1, 'student': 1, 'up': 1, 'much': 1, 'angry': 1, 'hurried': 1, 'help': 1, 'paying': 1, 'into': 1, 'me': 1, 'purchase': 1, 'after': 1, 'discovered': 1, 'not': 1, 'going': 1, 'faithful': 1, 'vehicle': 1, 'dispute': 1, 'trouble': 1, 'off': 1, 'expalin': 1, 'contacted': 1, 'dropped': 1, 'you': 1, 'issue': 1, 'being': 1, 'this': 1, 'over': 1, 'could': 1, 'deliquint': 1, 'switched': 1, 'situation': 1, 'back': 1, 'contact': 1, 'resolve': 1, 'maybe': 1, 'paid': 1, 'at': 1, 'tried': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Creating an object of count vectorizer\n",
    "count_vectorizer = Counter(bag_of_words_lower)\n",
    "\n",
    "print(count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vectorization using sklearn\n",
    "\n",
    "***\n",
    "We did manage to convert our list of words to numbers. However the problem still remain unresolved. \n",
    "\n",
    "**How do we apply our algorithm to this?**\n",
    "\n",
    "**Could we convert every word to a feature (or column) and the count associated with it to it's value and then apply a Classification algorithm to it? Something like below?**\n",
    "\n",
    "<img src=\"../images/img2.png\">\n",
    "\n",
    "** This looks very similar to one-hot encoding and is a typical method of applying ML to text data**. \n",
    "\n",
    "This is similar to one-hot in the way that when we add the second row, and the the third row and subsequent rows, the features or the columns will increase as more and more words come in and there will be words which do not appear in say the first_complaint, the vectorizer will automatically assign 0 to those words. **Hence the number of features will be equal to the total number of unique words in all the complaints combined and the values for those features will be the count of those words in that particular complaint.**\n",
    "\n",
    "A normal classification algorithm can now be applied where X is all the features except the Product column and y is the Product column. \n",
    "\n",
    "We could also use the sklearn's Count Vectorizer method and convert all the text into numbers in a single step. \n",
    "\n",
    "Let's do it for the first row\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Initialising a CountVectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Storing the first row in Text\n",
    "txt = [data[\"X\"].iloc[0]]\n",
    "\n",
    "#Printing the first row\n",
    "print (\"\\nFirst Row:\\n\",txt)\n",
    "\n",
    "#Fitting the CountVectorizer objext\n",
    "cv.fit(txt)\n",
    "\n",
    "#Transforming the first row\n",
    "vector = cv.transform(txt)\n",
    "\n",
    "\n",
    "print (\"\\nVector Shape:\\n\", vector.shape)\n",
    "\n",
    "#Storing the values of vector in array format\n",
    "vector_values = vector.toarray()\n",
    "\n",
    "print(\"\\nVector Values:\\n\",vector_values)\n",
    "\n",
    "print(\"These are the counts of the 69 unique words in our first complaint.\")\n",
    "\n",
    "print (\"\\nCount Vectorizer Vocabulary:\\n\",cv.vocabulary_) \n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "\n",
    "First Row:\n",
    "['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.']\n",
    "\n",
    "Vector Shape:\n",
    "(1, 69)\n",
    "\n",
    "Vector Values:\n",
    "[[1 5 1 1 1 2 1 2 1 1 2 1 1 1 1 4 2 1 1 1 1 1 1 1 1 1 4 2 1 1 1 1 2 1 2 1\n",
    "  1 1 4 3 1 1 1 1 1 1 1 1 2 1 2 1 1 3 8 1 1 1 5 3 1 1 1 1 5 2 3 3 1]]\n",
    "These are the counts of the 69 unique words in our first complaint.\n",
    "\n",
    "Count Vectorizer Vocabulary:\n",
    "{'when': 65, 'my': 38, 'loan': 34, 'was': 64, 'switched': 52, 'over': 43, 'to': 58, 'navient': 39, 'never': 40, 'told': 59, 'that': 53, 'had': 26, 'deliquint': 17, 'balance': 5, 'because': 6, 'with': 66, 'xxxx': 67, 'did': 18, 'not': 41, 'going': 25, 'purchase': 46, 'vehicle': 63, 'discovered': 19, 'credit': 15, 'score': 48, 'been': 7, 'dropped': 21, 'from': 24, 'the': 54, 'into': 30, 'have': 27, 'faithful': 23, 'at': 3, 'paying': 45, 'student': 51, 'company': 11, 'delinquency': 16, 'contacted': 13, 'resolve': 47, 'this': 57, 'issue': 31, 'you': 68, 'and': 1, 'kept': 33, 'being': 8, 'just': 32, 'contact': 12, 'bureaus': 10, 'expalin': 22, 'situation': 49, 'maybe': 35, 'they': 56, 'could': 14, 'help': 28, 'me': 36, 'so': 50, 'angry': 2, 'hurried': 29, 'paid': 44, 'off': 42, 'then': 55, 'after': 0, 'tried': 60, 'dispute': 20, 'much': 37, 'trouble': 61, 'bringing': 9, 'back': 4, 'up': 62}\n",
    "\n",
    "```\n",
    "\n",
    "The vocabulary only specifies the index of the word and the not the counts.\n",
    "\n",
    "Comparing the vector values with vocabulary helps in identifying the word count.\n",
    "\n",
    "So for the word with index 1 is `and` we see its value is 5. That means the count of the word `and` is 5. \n",
    "\n",
    "```python\n",
    "\n",
    "# #Converting the vector values to list\n",
    "vector_values = vector_values.tolist()[0]\n",
    "print (vector_values)\n",
    "\n",
    "\n",
    "print (\"count value of the word at index 22\")\n",
    "print (vector_values[22]) \n",
    "\n",
    "print (\"count value of the word at index 34, the word is 'loan'\")\n",
    "print (vector_values[34]) \n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "[1, 5, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 8, 1, 1, 1, 5, 3, 1, 1, 1, 1, 5, 2, 3, 3, 1]\n",
    "\n",
    "count value of the word at index 22\n",
    "1\n",
    "\n",
    "count value of the word at index 34, the word is 'loan'\n",
    "2\n",
    "```\n",
    "***\n",
    "\n",
    "Seeing the cv.vocabulary_ dictionary we see that the word is 'loan' and it's value in the vector values list is 2.\n",
    "\n",
    "<img src=\"../images/img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Count vectorization on a dataframe\n",
    "\n",
    "We have just seen how we can use the count vectorizer on a text snippet. Most of the times the data is present in a dataframe. Let's try implementing the count vectorizer on a dataframe. \n",
    "\n",
    "We will use the count vectorizer to transform the X column of the dataframe which corresponds to the text paragraph and make a new dataframe with these features and the product column. \n",
    "\n",
    "We will consider just the top 3 rows of the entire dataframe to aid better understanding and run it over the entire dataframe later.\n",
    "\n",
    "```python\n",
    "#Importing count vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Initializing the Count Vectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "\n",
    "#Creating a dataframe \"all text\" with the first 3 rows of data\n",
    "all_text = data[\"X\"][:3]\n",
    "all_text = pd.DataFrame(all_text)\n",
    "\n",
    "#Renaming the column for that dataframe (has only one column) to \"Text\"\n",
    "all_text.columns = [\"Text\"]\n",
    "\n",
    "#Converting to lower case\n",
    "all_text[\"Text\"] = all_text['Text'].str.lower()\n",
    "\n",
    "#Fitting the Count vectorizer all text\n",
    "cv.fit(all_text[\"Text\"])\n",
    "\n",
    "#Transforming \"Text\"\n",
    "vector = cv.transform(all_text[\"Text\"])\n",
    "\n",
    "#Transforming the vector to array\n",
    "vector_values_array = vector.toarray()\n",
    "\n",
    "#Converting the text to numbers - The transform function does this. \n",
    "vector_values_list = vector_values_array.tolist()\n",
    "\n",
    "#Length of vector value list\n",
    "print (\"No of rows of vector value list\\n\", len(vector_values_list)) \n",
    "\n",
    "\n",
    "print(\"\\nThe first row of vector_values_list\\n\",vector_values_list[0])\n",
    "\n",
    "print (\"\\nNo of words in first row\\n\",len(vector_values_list[0]))\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "No of rows of vector value list:\n",
    "3\n",
    "\n",
    "The first row of vector_values_list:\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 4, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 4, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 3, 8, 0, 0, 1, 1, 0, 1, 0, 5, 3, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 1, 0]\n",
    "\n",
    "No of words in first row:\n",
    "214\n",
    "```\n",
    "\n",
    "214 is the number of unique words in all 3 rows combined. This value will be constant for every list element in the vector_values_list because all the unique words in the entire dataframe have been converted to features and the values for these features per row depends on the count of those words in that row, 0 in case the word does not exist in the row. \n",
    "\n",
    "\n",
    "We can confirm that with the following\n",
    "\n",
    "***\n",
    "```python\n",
    "print(\"\\nThe second row of vector_values_list\\n\",vector_values_list[1])\n",
    "\n",
    "print (\"\\nNo of words in second row\\n\",len(vector_values_list[1]))\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "The second row of vector_values_list\n",
    "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "No of words in second row\n",
    "214\n",
    "```\n",
    "***\n",
    "\n",
    "<!--```python\n",
    "print(\"\\nThe third row of vector_values_list\\n\",vector_values_list[2])\n",
    "\n",
    "print (\"\\nNo of words in third row\\n\",len(vector_values_list[2]))\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "The third row of vector_values_list\n",
    " [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 5, 0, 1, 1, 1, 0, 2, 0, 1, 4, 1, 1, 0, 1, 0, 3, 1, 0, 0, 3, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 0, 0, 1, 0, 0, 4, 1, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 3, 1, 0, 4, 1, 1, 1, 1, 3, 0, 3, 0, 0, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 6, 1, 3, 0, 0, 1, 1, 4, 2, 3, 1, 3, 1, 4, 0, 1, 1, 1, 0, 0, 3, 0, 5, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 4, 6, 3, 0, 1, 7, 1, 8, 0, 9, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 0, 1, 6, 1, 3, 3, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 5, 3]\n",
    "\n",
    "No of words in third row\n",
    " 214\n",
    "```-->\n",
    "\n",
    "As you can see that every document has been converted to a fixed length vector of 214 words, and have values corresponding to the occurrences of those words in the particular document.\n",
    "\n",
    "Let's now get the y values for these 3 rows\n",
    "\n",
    "```python\n",
    "\n",
    "#Creating a dataframe for target\n",
    "labels = pd.DataFrame(data[\"y\"][:3])\n",
    "labels.columns = [\"labels\"]\n",
    "print(labels)\n",
    "```\n",
    "\n",
    "**Output**\n",
    "\n",
    "```python\n",
    "                        labels\n",
    "1                 \"Student loan\"\n",
    "2  \"Credit card or prepaid card\"\n",
    "7                     \"Mortgage\"\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "We will have to label encode these categories to numerize them. \n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels[\"labels\"] = le.fit_transform(labels[\"labels\"])\n",
    "print (labels)\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "    labels\n",
    "1       2\n",
    "2       0\n",
    "7       1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorising Dataset\n",
    "\n",
    "In this task we will try to implement the vectorisation on all rows and implement a logistic regression model on the vectorised dataframe\n",
    "\n",
    "### Data Vectorisation\n",
    "\n",
    "**For X**\n",
    "- Store the `\"X\"` column of dataframe `'data'`in a new dataframe `'all_text'`(i.e. instead of `data[\"X\"]` use `data[[\"X\"]]`)\n",
    "\n",
    "- Convert the values of X column of `all_text` into lowercase using `\"lower()\"` method.\n",
    "\n",
    "- Initialise a `\"CountVectorizer()\"` object and store it in `'cv'`\n",
    "\n",
    "- Apply the `\"fit_transform()\"` method of `'cv'` on `X` column and store the result in `'vector'`\n",
    "\n",
    "- Convert `'vector'` into an array using `\"toarray()\"` method and store the result in a new variable `'X'`\n",
    "\n",
    "**For y**\n",
    "\n",
    "- Store the `\"y\"` column of dataframe `'data'`in a new dataframe `'labels'`(i.e. instead of `data[\"y\"]` use `data[[\"y\"]]`)\n",
    "\n",
    "- We need to label encode the values. Therefore initialise a `\"LabelEncoder()\"` object and store it in `'le'`\n",
    "\n",
    "- Use the `fit_transform` method of `'le'` on column `\"y\"` of `'labels'` and store the results back in `y` column\n",
    "\n",
    "***\n",
    "If we now, include all the 335 rows and vectorize them and label it as the X, and the corresponding y labels, we can train a classification algorithm on the same, and figure out the accuracy. \n",
    "\n",
    "***\n",
    "\n",
    "### Model building\n",
    "\n",
    "\n",
    "- Split `'X'` and `'labels[\"y\"]'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.4` and `random_state = 42`\n",
    "\n",
    "\n",
    "- Initialise a logistic regression model with `LogisticRegression()` having `random_state=42` and save it to a variable called `'log_reg'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the accuracy score between `X_test` and `'y_test'` using the `'score()'` method and save it in a variable called `'acc'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.283097Z",
     "start_time": "2018-12-24T17:44:55.023123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4925373134328358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Running the exact same code as the earlier one on 335 rows\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#Subsetting 'X'\n",
    "all_text = data[[\"X\"]]\n",
    "\n",
    "#Converting 'X' to lower case\n",
    "all_text[\"X\"] = all_text['X'].str.lower()\n",
    "\n",
    "#Initialising a count vectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Creating the count vectorizer of our 'X' column\n",
    "vector =cv.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the count vectoriser to array\n",
    "X = vector.toarray()\n",
    "\n",
    "#Subsetting y\n",
    "labels = data[[\"y\"]]\n",
    "\n",
    "#Initialising a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Label encoding 'y' column\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model on train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score on test data\n",
    "acc = log_reg.score(X_test,y_test)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a 49% accuracy of predicting the product category. Which isn't really a good number. A look at the precision recall shows that the product category was heavily imblanaced, hence a few categories have not been detecetde by the algorithm. Before we rectify the same, let us first look at the distribtion of the y column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Removing Stopwords\n",
    "\n",
    "In the previous task, we have seen 49% accuracy of predicting the product category. Now the question we need to ask is - can we improve the accuracy further? The answer lies in dealing with stopwords. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Stop Words**\n",
    "\n",
    "It is important to realize that count vectorizer is essentially a Ranking algorithm, in the way that it gives a higher weight to words which have appeared more number of times. In other words, the key_value pair is nothing but the count of the words in the dataset. It does not assign any importance to the order in which the words have appeared in the sentences. \n",
    "\n",
    "Another point to be considered is the fact that words like \"a\",\"an\",\"the\" etc. will appear more number of times than the rest of the words as they are common articles. Using a Count vectorizer out of the box on a paragraph or a body of text will invariably give the highest count to these common words. Hence the words we are actually interested in will be underneath these words. One way to rectify this, is to remove these commonly occurring words. NLTK offers this functionality and has rightly defined these particular words as \"stopwords\" or words we wouldn't include in our bag of words. We will have to remove the punctuation as well, as you can see that our initial bag of words had the commas, full-stops and all of the other symbols as well. We will have to remove them as well, to avoid being vectorized. \n",
    "\n",
    "To see the list of stopwords, NLTK currently includes, we could check that by just running \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "print (set(stopwords.words('english')))\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "{'their', 'yourselves', 'yours', 'ours', \"isn't\", 'ourselves', 'off', 'herself', 'each', 'hadn', \"don't\", 'and', 'haven', 'be', 'how', 'won', 'more', 'll', \"needn't\", 'hers', 'then', 'in', 'the', 'shouldn', 'very', 'doing', \"you'll\", 'theirs', 'than', 'will', 'under', 'when', \"hadn't\", 'few', 'isn', 'not', 'was', 'has', 'here', 'any', 'just', \"didn't\", \"shouldn't\", 'd', 'ma', \"haven't\", 'that', \"doesn't\", 'your', 'what', 'him', 'out', 'being', 'at', 'into', 'some', 'doesn', 'such', 'his', 'he', 'which', 've', 'about', 'up', 'during', 'they', \"she's\", 'myself', 'having', 'm', \"it's\", 'aren', 'this', \"shan't\", 'should', 'have', 'these', 'because', \"weren't\", \"won't\", 'mightn', 'down', \"wasn't\", 'our', 'to', \"mustn't\", 'but', 'are', 'had', 'y', 'same', 'shan', 'we', 'too', 'couldn', 'other', 'after', 'its', 'me', 'why', 'own', 'whom', 'for', \"wouldn't\", 'before', \"you're\", 'do', 'a', \"mightn't\", 'above', 'wouldn', 'who', 'no', 'of', 'an', 'nor', 'there', 'can', 'between', 'were', 'where', 'now', 'through', 'below', \"you've\", 'o', 'or', 'again', 'don', 'did', 'her', 'from', \"that'll\", 're', 'my', 'himself', 'further', 'wasn', 'with', 'until', 't', 'them', 'if', 'it', \"hasn't\", 'been', 'so', 'while', 'hasn', 'itself', 'is', 'weren', 'those', 'themselves', 'am', \"couldn't\", 'by', 'on', 'she', 'over', 'most', 'does', 'only', 'i', 'ain', 'as', 'once', 'all', 'mustn', 'you', 's', 'didn', 'needn', \"should've\", \"aren't\", 'against', 'both', \"you'd\", 'yourself'}\n",
    "```\n",
    "The punctuation list can be derived as follows. \n",
    "\n",
    "```python\n",
    "from string import punctuation\n",
    "print (list(punctuation))\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "['!', '\"', '#', 'dollar', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "```\n",
    "We can also add our own list of stop words we want to remove from our body of text. Different domains can have different stopwords - for example, if we are classifying medical articles into different subdomains like orthopedic and neurology, then the word `medicine` would be a stopword for our case. So we can add `medicine` to the set of stopwords in the following manner \n",
    "\n",
    "```python\n",
    "custom_set_of_stopwords = set(stopwords.words('english')+list(punctuation)+[\"medicine\"])\n",
    "                             \n",
    "```\n",
    "This will include the word `medicine` as a stop word and remove the same from our body of text before vectorizing it. \n",
    "\n",
    "You can check it also\n",
    "```python\n",
    "print (\"medicine\" in custom_set_of_stopwords)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "True\n",
    "```\n",
    "\n",
    "If we re-do the exercise for prediction removing the stopwords, the accuracy would increase, as now we are not giving any weight to meaningless words but to words which actually matter. \n",
    "\n",
    "**Python Implementation of Stopwords**\n",
    "\n",
    "\n",
    "```python\n",
    "#Storing the first complaint\n",
    "first_complaint = data.iloc[0][0]\n",
    "\n",
    "print(\"\\nFirst Complaint:\\n\",first_complaint)\n",
    "\n",
    "bag_of_words = word_tokenize(first_complaint)\n",
    "\n",
    "print (\"\\nBag of words of first complaint:\\n\",bag_of_words)\n",
    "print(\"\\nLen of bag of words:\\n\",len(bag_of_words))\n",
    "\n",
    "#Removing stopwords\n",
    "bow_stopwords_removed = [x for x in first_complaint_bow if x not in custom_set_of_stopwords]\n",
    "\n",
    "print (\"\\nBag of words with stopwords removed:\\n\",bow_stopwords_removed)\n",
    "\n",
    "print(\"Len of bag of words with stopwords removed:\\n\",len(bow_stopwords_removed))\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "\n",
    "First Complaint:\n",
    "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
    "\n",
    "Bag of words of first complaint:\n",
    "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n",
    "\n",
    "Len of bag of words:\n",
    "137\n",
    "\n",
    "Bag of words with stopwords removed:\n",
    "['loan', 'switched', 'navient', 'never', 'told', 'deliquint', 'balance', 'xxxx', 'going', 'purchase', 'vehicle', 'discovered', 'credit', 'score', 'dropped', 'xxxx', 'xxxx', 'faithful', 'paying', 'student', 'loan', 'told', 'navient', 'company', 'delinquency', 'contacted', 'navient', 'resolve', 'issue', 'kept', 'told', 'contact', 'credit', 'bureaus', 'expalin', 'situation', 'maybe', 'could', 'help', 'angry', 'hurried', 'paid', 'balance', 'tried', 'dispute', 'delinquency', 'credit', 'bureaus', 'much', 'trouble', 'bringing', 'credit', 'score', 'back']\n",
    "\n",
    "Len of bag of words with stopwords removed:\n",
    "54\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T15:06:54.633716Z",
     "start_time": "2018-12-23T15:06:54.629159Z"
    }
   },
   "source": [
    "# Stopword removal\n",
    "\n",
    "Let's remove the stopwords from our entire dataset\n",
    "\n",
    "### Stopword handling\n",
    "- Initialise a `\"CountVectorizer()\"` object with parameter `stop_words= \"english\"` and store it in `'cv_stop'`\n",
    "\n",
    "- Apply the `\"fit_transform()\"` method of `'cv'` on `X` column and store the result in `'vector_stop'`\n",
    "\n",
    "- Convert `'vector_stop'` into an array using `\"toarray()\"` method and store the result in a new variable `'X_stop'`\n",
    "\n",
    "### Model building\n",
    "\n",
    "- Split `'X_stop'` and `'labels[\"y\"]'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.4` and `random_state = 42`\n",
    "\n",
    "\n",
    "- Initialise a logistic regression model with `LogisticRegression()` having `random_state=42` and save it to a variable called `'log_reg'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the accuracy score between `X_test` and `'y_test'` using the `'score()'` method and save it in a variable called `'stop_acc'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.416164Z",
     "start_time": "2018-12-24T17:44:55.285477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5597014925373134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Initialising the count vectorizer with stop words parameter\n",
    "cv_stop = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Creating the count vectorizer of our 'X' column\n",
    "vector_stop = cv_stop.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the count vectoriser to array\n",
    "X_stop = vector_stop.toarray()\n",
    "\n",
    "#Splitting the data to train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_stop,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initalising a logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model on train\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score on test data\n",
    "stop_acc = log_reg.score(X_test,y_test)\n",
    "print (stop_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Advanced vectorization with TF-IDF \n",
    "\n",
    "Description: In this chapter, we will talk about how to convert text into tf-idf vectors for better classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Introduction to TF-IDF\n",
    "\n",
    "In the last tutorial we saw how text was converted to numerics using a count vectorizer. \n",
    "\n",
    "In other words, a count vectorizer, counts the occurences of the words in a document and all the documents are considered independent of each other. Very similar to a one hot encoding or pandas getdummies function. However in cases where multiple documents are involved, count vectorizer still does not assume any interdependence between the documents and considers each of the documents as a seperate entity. \n",
    "\n",
    "It does not rank the words based on their importance in the document, but just based on whether they exist or not. This is not a wrong approach, but it intuitively makes more sense to rank words based on their importance in the document right? In fact, the process of converting, text to numbers should essentially be a ranking system of the words so that the documents can each get a score based on what words they contain. All words cannot have the same imprtance or relevance in the document right?\n",
    "\n",
    "There are two ways to approach document similarity:\n",
    "\n",
    "\n",
    "1. TF-IDF Score\n",
    "\n",
    "2. Cosine Similarity\n",
    "\n",
    "Let's look at them one by one.\n",
    "\n",
    "\n",
    "#### TF-IDF!!\n",
    "\n",
    "TF-IDF or Term Frequency and Inverse Document Frequency is kind of the holy grail of ranking metrics to convert text to numbers. Consider the count vectorizer as a metric which just counts the occurences of words in a document. \n",
    "\n",
    "** The ranking system in a count vectorizer is purely occurence based on a single document only!**\n",
    "\n",
    "TF-IDF takes it a step further and ranks the words based not just on their occurences in one document but across all the documents. Hence if CV or Count vectorizer was giving more importance to words because they have appeared multiple times in the document, TF-IDF will rank them high if they have appeared only in that document, meaning that they are rare, hence higher importance and lower if they have appeared in all or most documents, because they are more common, hence lower ranking. \n",
    "\n",
    "Consider a scenario where there are 5 documents and all are talking aout football. The word football would have appeared multiple times in each document. CV is going to rank football consistently high and infact give the word football a different value across all 5 documents based on how many times that word has appeared in that document. In other words, it is assuming, that the more number of times a word appears, the more important it is. That is exactly what the TF or the Term Frequency component in TF-IDF does. \n",
    "\n",
    "IDF on the other hand now is the dominating factor in TFIDF which is going to find out the number of times football has also appeared in the other 4 documents except for the one it is currently seeing. If football has also appeared in rest of the documents, it means that though football is important to that one document based on the number of occurences, considering it has appeared in the rest as well, it is not that rare or more common, hence the importance now is going to reduce instead of going high!\n",
    "\n",
    "**The ranking system is across the entire corpus or all documents.  It is not a single document based metric!**\n",
    "\n",
    "We have seen how CV is calculated for a word in a document. Let us now see how TF IDF is...\n",
    "\n",
    "The tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "#### Example\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "**Python Implementation of TF-IDF**\n",
    "Let us now take the first 3 complaints and run a TF-IDF vectorizer on the same. So, in this case, the 3 complaints are our 3 documents. and instead of a CV which considers each document independent of each other and just calculates the count of every word in the document. now the corpus will be the sum total of both documents. \n",
    "\n",
    "```python\n",
    "complaint_1 = data[\"X\"].iloc[0]\n",
    "complaint_2 = data[\"X\"].iloc[1]\n",
    "complaint_3 = data[\"X\"].iloc[2]\n",
    "\n",
    "print (\"Complaint 1: \", complaint_1)\n",
    "\n",
    "print (\"\\nComplaint 2: \", complaint_2)\n",
    "\n",
    "print (\"\\nComplaint 3: \", complaint_3)\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "Complaint 1:  When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
    "\n",
    "Complaint 2:  I tried to sign up for a spending monitoring program and Capital One will not let me access my account through them\n",
    "\n",
    "Complaint 3:  My mortgage is with BB & T Bank, recently I have been investigating ways to pay down my mortgage faster and I came across Biweekly Mortgage Calculator on BB & T 's website. It's a nice, easy to use calculator that you plug in your interest rate, mortgage amount, mortgage term, and payment type and it calculates your accelerated bi-weekly payment for you and shows you how much quicker you can pay down your loan. Ours figured out to pay off a 30 year mortgage in 26.4 years ... quite a savings! \n",
    "I called BB & T 's customer service number to inquire how I get set up on this payment plan. I was told they do not offer that type of payment plan, but I could send in my payments bi-weekly but it would not be applied until the full amount was received. ( the money would sit in a \" holding account '' until the full payment amount was collected ). I ended up calling back a few days later thinking the rep I was talking to didn't understand what I wanted to do or was not knowledgeable of this program. I got the SAME ANSWER! \n",
    "I then asked for the corporate BB & T office number where I could speak to someone that was knowledgeable of this product. After 3 days I received a phone call back from a corporate manager stating they do not offer this product, and they were \" checking into why this is on their website ''. She stated they do have a few customers that make bi-weekly payments, but they no longer offer this service. \n",
    "I don't understand how they can have this active link on their website under their Financial Planning Center tab to mislead customers when all they say is \" I'm sorry, I know you're upset about this '' Sounds like false advertising to me! \n",
    "https : //www.bbt.com/XXXX\n",
    "```\n",
    "\n",
    "Let's find out the Tf-Idf score \n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "sents = [complaint_1, complaint_2, complaint_3]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "\n",
    "vectorizer.fit(sents)\n",
    "\n",
    "vector = vectorizer.transform(sents)\n",
    "\n",
    "print(\"Shape of the vectorized sentence:\",vector.shape)\n",
    "\n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "\n",
    "print(\"The tf-idf score of first five elements:\",vector_values[:5])\n",
    "\n",
    "\n",
    "# Converting the tf-idf score with the word into a dictionary\n",
    "import operator\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "\n",
    "print(\"Dictionary of words with tf-idf score:\\n\", d)\n",
    "\n",
    "#Sorting this dictionary by value in the descending order to see the ranking\n",
    "print(\"Sorted dictonary:\\n\")\n",
    "print (sorted(d.items(), key=operator.itemgetter(1), reverse = True))\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "Shape of the vectorized sentence: (3, 214)\n",
    "\n",
    "The tf-idf score of first five elements: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "Dictionary of words with tf-idf score:\n",
    "{'26': 0.0, '30': 0.0, 'about': 0.0, 'accelerated': 0.0, 'access': 0.0, 'account': 0.0, 'across': 0.0, 'active': 0.0, 'advertising': 0.0, 'after': 0.05253580411952334, 'all': 0.0, 'amount': 0.0,\n",
    "  ..............................................................................................\n",
    " }\n",
    "    \n",
    "Sorted dictonary:\n",
    "\n",
    "[('the', 0.42028643295618673), ('credit', 0.27631307611219824), ('had', 0.27631307611219824), ('was', 0.2626790205976167), ('navient', 0.20723480708414868), ('and', 0.20399369240069398), ..................................................................................................\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "We can see that the model learns to give lesser importance to words like is,it,in etc;. Unfortunately, it also gives a low importance to important words like financial, mortgage and a fairly high importance to unwanted words like the, was. It does give higher importance to words such as credit. And that is because TF-DF works better with larger corpuses. Just like a machine learning model, the larger the data, the better the model.  With a larger corpus, these issues would be resolved when a lot more documents would have words like financial but not the.\n",
    "\n",
    "Rerunning this for about 100 documents, we see that the ranking is completely different. \n",
    "\n",
    "```python\n",
    "sents=[]\n",
    "for x in range(100):\n",
    "    sents.append(data[\"X\"].iloc[x])\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents called sents\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(sents)\n",
    "vector = vectorizer.transform(sents)\n",
    "vector.shape \n",
    "vector_values = vector.toarray().tolist()[0]\n",
    "\n",
    "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "words = [x[0] for x in sorted_x]\n",
    "d = dict(zip(words,vector_values))\n",
    "print(\"Sorted dictionary: \\n\")\n",
    "print ((sorted(d.items(), key=operator.itemgetter(1), reverse = True))[:20])\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "Sorted dictionary: \n",
    "\n",
    "[('navient', 0.35431369455512246), ('the', 0.24041090745878946), ('delinquency', 0.23620912970341496), ('had', 0.20963581996093744), ('told', 0.19801036084460227), ('bureaus', 0.19189624902422267),\n",
    "```\n",
    "\n",
    "You can notice that \"the\" has moved down from 0.42 to 0.27. Navient has increased from 0.20 to 0.35. So has bureaus from 0.13 to 0.19. As we include more and more sentences, the words whch have appeared more and more frequently across all the documents, such as \"the\" are moving down in value, and words like bureau and navient, which have appeared far lesser number of times have started increasing. Which reiterates the point we had. TF-DF works better with larger corpuses. Just like a machine learning model, the larger the data, the better the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification with TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous topic, we have seen how the ranking of features changes as more data is available to the algorithm. Just like, how we have done with count vectorization we can use the tfidf features as input for a classification algorithm. \n",
    "\n",
    "We use the `TfidfVectorizer` and `fit_transform()` method that we have seen in the previous instances. We will apply a logistic regression model on the data and get the classification accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF- IDF score\n",
    "\n",
    "Let's run our initial Logistic Regression model using a tf-idf and see if there is a difference in the accuracies. \n",
    "\n",
    "#### TF-IDF scoring\n",
    "- Initialise a `\"TfidfVectorizer()\"` object with parameter `stop_words= \"english\"` and store it in `'tfidf'`\n",
    "\n",
    "- Apply the `\"fit_transform()\"` method of `'tfidf'` on `X` column and store the result in `'vector_tfidf'`\n",
    "\n",
    "- Convert `'vector_tfidf'` into an array using `\"toarray()\"` method and store the result in a new variable `'X_tfidf'`\n",
    "\n",
    "#### Model building\n",
    "\n",
    "- Split `'X_tfidf'` and `'labels[\"y\"]'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.4` and `random_state = 42`\n",
    "\n",
    "\n",
    "- Initialise a logistic regression model with `LogisticRegression()` having `random_state=42` and save it to a variable called `'log_reg'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the accuracy score between `X_test` and `'y_test'` using the `'score()'` method and save it in a variable called `'tfidf_acc'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.519442Z",
     "start_time": "2018-12-24T17:44:55.418624Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43283582089552236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Initialising the tf-idf model\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Vectorizing the 'X' column\n",
    "vector =tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model with train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "tfidf_acc = log_reg.score(X_test,y_test)\n",
    "print (tfidf_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tfidf vectorizer with more data\n",
    "\n",
    "We can see that the overall accuracy of the model is **low** compared to the initial model we built with count vectorizer model. Let us see with additional data, what happens to the accuracy! \n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "#Reading the data\n",
    "df = pd.read_pickle(\"../data/a2.pkl\")\n",
    "\n",
    "#Keeping the relevant columns\n",
    "df = df[[\"Consumer complaint narrative\", \"Product\"]] \n",
    "df.columns = [\"X\",\"y\"]\n",
    "df = df.dropna()\n",
    "df = df.iloc[:2000]\n",
    "print(\"No. of rows in data: \",df.shape[0])\n",
    "\n",
    "\n",
    "#Initialising the tf-idf model\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Vectorizing the 'X' column\n",
    "vector =tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model with train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "acc = log_reg.score(X_test,y_test)\n",
    "print (\"Accuracy Score: \"acc)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "No. of rows in data:  2000\n",
    "Accuracy Score: 0.5885608856088561\n",
    "```\n",
    "From 43% at 359 rows, to 59% at 2000 rows, we are starting to see how TF-IDF works better with larger data sets. The last value for the word \"the\" was around 0.27. Just out of curiosity, let's see the value of the word \"the\" now at 2000 docs\n",
    "\n",
    "```python\n",
    "[('navient', 0.36025047479827665), ('delinquency', 0.26853246892091703), ('the', 0.21010966235398107), ('had', 0.19406511916152228), ('deliquint', 0.17884877931376503), ('expalin', 0.17884877931376503), ('faithful', 0.17884877931376503), ('hurried', 0.17884877931376503), ('told', 0.1715249118834407), ('was', 0.16955513647222115), ('score', 0.16237392880244142), ('angry', 0.15785573774281664), ('bureaus', 0.15439765356279928), ('switched', 0.14197511484650308), ('credit', 0.14078592528264697), ('just', 0.1406764594136565), ('bringing', 0.13979147323891608), ('balance', 0.13890249109697195), ('trouble', 0.13596411449573909), ('maybe', 0.1285083957562635)]\n",
    "```\n",
    "\n",
    "\"the\" is at 0.21 at 2000 documents. \"And\" has been pushed down, expalin has come up, deliquent has increased etc; a few words have been pushed down as well, based on their importance across all documents not just each document alone. One thing to note is that sometimes Count Vectorizer would work better than the TF-IDF Vectorizer based on the distribution of the words. \n",
    "\n",
    "\n",
    "**Cosine Similarity**\n",
    "\n",
    "Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. This is calculated as\n",
    "\n",
    "![](../images/cos_sim.png)\n",
    "\n",
    "Here vectors can be either be bag of words with either TF (term frequency) or TF-IDF (term frequency- inverse document frequency).\n",
    "\n",
    "Let's understand it better with an example\n",
    "\n",
    "Consider the two sentences:\n",
    "\n",
    "- a. Amy likes pie more than Linda likes pie\n",
    "\n",
    "- b. Linda likes cake more than Amy likes cake\n",
    "\n",
    "\n",
    "Following will be the unique words:\n",
    "\n",
    "\"Amy\", \"likes\" \"pie\" , \"more\" , \"than\", \"Linda\", \"cake\"\n",
    "\n",
    "Following will be their respective tf vectors:\n",
    "\n",
    "a= [1,2,2,1,1,1,0]\n",
    "\n",
    "b= [1,2,0,1,1,1,2]\n",
    "\n",
    "Since term frequency counts favors the sentences that are longer, let's normalize the term frequencies with the respective magnitude.\n",
    "\n",
    "Normalising(L2 normalising) them we will get:\n",
    "\n",
    "a = [1/12,2/12,2/12,1/12,1/12,1/12,0/12]\n",
    "\n",
    "b = [1/12,2/12,0/12,1/12,1/12,1/12,2/12]\n",
    "\n",
    "\n",
    "Calculating cosine similarity is now simply finding the dot product of the vectors:\n",
    "\n",
    "Cos similarity\n",
    "= (1/12 x 1/12) + (2/12 x 2/12) + (2/12 x 0/12) + (1/12 x 1/12) + (1/12 x 1/12) + (1/12 x 1/12) + (0/12 x 2/12) = 1/12 + 4/12 + 0 + 1/12 + 1/12 + 1/12 + 0= 8/12= 0.66  \n",
    "\n",
    "\n",
    "From the above example, we can ses cosine similarity is good for cases where duplication matters while analyzing text similarity.\n",
    "\n",
    "Though TF-IDF is the popular method, you can still attempt cosine similarity to validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: More Classifiers for Text\n",
    "\n",
    "Description: In this chapter, we will look at other different classifiers that can be used for classifying text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes classifier is a linear classifier based on the Bayes' theorem. The term naive comes from the assumption of considering all features in a dataset are mutually independent. The independent assumption is generally violated in real datasets, but the naive Bayes Classifier still tends to perform very well.\n",
    "\n",
    "For a document d and class c, it is defined as:\n",
    "$P(c|d)$ is the probability of a document d belonging to class c. Or in other words, given the content of the document, what is the probability that it belongs to class c?\n",
    "\n",
    "On applying Bayes rule, we get\n",
    "\n",
    "$posterior = \\frac{prior\\ \\times\\ conditional\\ probability}{evidence}$ \n",
    "\n",
    "$P(c|d)= \\frac{P(d|c).P(c)}{P(d)}$\n",
    "\n",
    "P(d|c) means the probability of observing the document d given that it belongs to class c. If we consider the document to be represented by the words, and we assume the words to be independent of each other. Then\n",
    "$P(d|c) = \\prod_{i=1}^n P(w_i|c)$\n",
    "\n",
    "So, it can be interpreted as the probability of observing the word $w_i$ in the document, given that it belongs to class c. Since all probabilities will have P(d) as their denominator, we can eliminate the denominator.\n",
    "Resulting in the following formula\n",
    "\n",
    "$P(c|d) = P(d|c) \\times P(c) = \\prod_{i=1}^n P(w_i|c) \\times P(c) $ \n",
    "\n",
    "Being easy to implement and fast, naive Bayes classifiers are used in many different fields including classification of RNA sequences and spam filtering.\n",
    "\n",
    "Let's understand it better with an example of spam filtering: \n",
    "\n",
    "For a spam classification problem, we have two classes - `ham` and `spam` (ham means not spam). Given any email, we need to find P(ham|email) and P(spam|email). Whichever probability is higher - we assign that label to the email. Consider, \n",
    "\n",
    "$$P(spam|email)= P(email|spam) \\times P(spam)$$\n",
    "\n",
    "$$P(spam) = \\frac{\\text{number of emails belonging to spam class}}{\\text{total number of emails in the corpus}}$$\n",
    "$$P(email|spam)= \\prod_{i=1}^n P(w_i|spam)\\ where\\ w_i\\in email$$\n",
    "Let us take a word `lottery` for example that is present in the text of the email.\n",
    "\n",
    "$$P(lottery|spam) = \\frac{\\text{number of times lottery appears in spam email}}{\\text{sum of number of times all words of vocabulary appear in spam email}}$$\n",
    "\n",
    "Intuitively, the word `lottery` would appear more in spam emails than in the ham emails, and overall increases the probability that email is spam. For all the words we choose as features, we calculate the above probability and aggregate it to get probability of observing the email. If P(spam|email) > P(ham|email), we allocate the label `spam` to the email or `ham` otherwise.  \n",
    "\n",
    "Now that we have understood the basic intuition of how Naive Bayes works, let us try running a Naive Bayes classifier. We will be using the TF-IDF method, exactly the same way we ran a logistic regression model in the earlier section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "- Load the dataset from `'path'`(given) using the `read_csv()` method from pandas and store it in `'data'`. \n",
    "\n",
    "- Drop nan values from the entire dataframe `data` using `\"dropna()\"` and save it back to `'data'`\n",
    "\n",
    "- Subset the dataframe  `'data'` to only include `\"Consumer complaint narrative\"` and  `\"Product\"` and store this dataframe subset back in `'data'`\n",
    "\n",
    "- Subset the dataframe `'data'` to only include first `2000` rows and store this dataframe subset back in `'data'`\n",
    "\n",
    "- Rename the column `\"Consumer complaint narrative\"` to `\"X\"` and `\"Product\"` to `\"y\"` by assigning `[\"X\",\"y\"]` to `data.columns` \n",
    "\n",
    "### TF-IDF vectoriser\n",
    "- Initialise a `\"TfidfVectorizer()\"` object with parameter `stop_words= \"english\"` and store it in `'tfidf'`\n",
    "\n",
    "- Apply the `\"fit_transform()\"` method of `'tfidf'` on `X` column and store the result in `'vector_tfidf'`\n",
    "\n",
    "- Convert `'vector_tfidf'` into an array using `\"toarray()\"` method and store the result in a new variable `'X_tfidf'`\n",
    "\n",
    "\n",
    "**For y**\n",
    "\n",
    "- Store the `\"y\"` column of dataframe `'data'`in a new dataframe `'labels'`(i.e. instead of `data[\"y\"]` use `data[[\"y\"]]`)\n",
    "\n",
    "- We need to label encode the values. Therefore initialise a `\"LabelEncoder()\"` object and store it in `'le'`\n",
    "\n",
    "- Use the `fit_transform` method of `'le'` on column `\"y\"` of `'labels'` and store the results back in `y` column\n",
    "\n",
    "\n",
    "### Model building\n",
    "\n",
    "\n",
    "- Split `'X'` and `'labels[\"y\"]'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.4` and `random_state = 42`\n",
    "\n",
    "\n",
    "- Initialise a naive bayes model with `MultinomialNB()` having `random_state=42` and save it to a variable called `'nb'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_train'` and `'y_train'` using the `'fit()'` method.\n",
    "\n",
    "\n",
    "- Find out the accuracy score between `X_test` and `'y_test'` using the `'score()'` method and save it in a variable called `'nb_acc'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:55.966601Z",
     "start_time": "2018-12-24T17:44:55.521998Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/greyatom/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42461964038727523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "path = \"../data/a2.pkl\"\n",
    "\n",
    "# reading the data\n",
    "data = pd.read_pickle(path)\n",
    "\n",
    "# keeping the relevant columns\n",
    "data = data[[\"Consumer complaint narrative\", \"Product\"]]\n",
    "\n",
    "# renaming the columns\n",
    "data.columns = [\"X\", \"y\"]\n",
    "\n",
    "# dropping the nan values\n",
    "data = data.dropna()\n",
    "\n",
    "# choosing the first 2000 documents\n",
    "data = data.iloc[:2000]\n",
    "\n",
    "# X\n",
    "\n",
    "# Subsetting 'X' column\n",
    "all_text = data[[\"X\"]]\n",
    "\n",
    "# Converting the 'X' column to lower case\n",
    "all_text[\"X\"] = all_text['X'].str.lower()\n",
    "\n",
    "# Initialising a tfidf vectorizer object with stopwords\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Vectorizing the 'X' column\n",
    "vector = tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "# Converting vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "# y\n",
    "\n",
    "# Subsetting 'y' column\n",
    "labels = data[[\"y\"]]\n",
    "\n",
    "# Initialising label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Label encoding 'y' column\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, labels[\"y\"], test_size=0.4, random_state=42)\n",
    "\n",
    "# Initialsing a naive bayes classifier\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fitting the model on train data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Finding the accuracy score of model on test data\n",
    "nb_acc = nb.score(X_test, y_test)\n",
    "print(nb_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Handling imbalanced text data\n",
    "\n",
    "One of the fundamental reasons the model isn't giving a good accuracy can be deduced from the classification report, where we see that the recall of multiple categories is 0 and that means that the data isn't balanced well enough. Since a classification algorithm tends to predict the majority class unless, the output categories are more or less equally balanced, the error in predicting the majority class will increase as it classifies more and more data, and classifies them wrong, hence reducing the overall accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print (classification_report(y_test,nb.predict(X_test)))\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "0                 0.00      0.00      0.00        14\n",
    "1                 0.00      0.00      0.00        23\n",
    "2                 0.00      0.00      0.00        15\n",
    "3                 0.00      0.00      0.00        26\n",
    "4                 0.00      0.00      0.00        48\n",
    "5                 0.00      0.00      0.00        52\n",
    "6                 0.38      1.00      0.55       250\n",
    "7                 0.86      0.40      0.54       144\n",
    "8                 0.00      0.00      0.00        16\n",
    "9                 0.00      0.00      0.00         2\n",
    "10                1.00      0.01      0.03        78\n",
    "11                0.00      0.00      0.00         1\n",
    "12                0.00      0.00      0.00         6\n",
    "13                0.00      0.00      0.00         5\n",
    "14                0.00      0.00      0.00        30\n",
    "15                0.00      0.00      0.00        13\n",
    "\n",
    "avg / total       0.41      0.42      0.30       723\n",
    "```\n",
    "\n",
    "Let's reconcile the classification report with the label wise distribution of data. \n",
    "\n",
    "\n",
    "```python\n",
    "labels[\"y\"].value_counts() \n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```python\n",
    "6     628\n",
    "7     418\n",
    "10    182\n",
    "4     112\n",
    "5     104\n",
    "14     86\n",
    "1      77\n",
    "3      56\n",
    "0      33\n",
    "15     27\n",
    "8      26\n",
    "2      24\n",
    "12     18\n",
    "9       7\n",
    "13      5\n",
    "11      3\n",
    "Name: y, dtype: int64\n",
    "```\n",
    "\n",
    "We can see very clearly that the data set has heavily imbalanced labels. Labels 10,8,12 are underrepresented and hence the probability of the model catching those labels is less, hence the accuracy is going to suffer. As predicted, the dataset is heavily imbalanced. With Category 6 and 7 being over represented , while all else have a less than 10% weightage. \n",
    "\n",
    "We might have to oversample the under-represented categories. \n",
    "\n",
    "**Random Oversampling**\n",
    "\n",
    "Since we already have less data, the sampling method we should use is oversampling. We have already covered Random Oversampling. \n",
    "\n",
    "To refresh,\n",
    "\n",
    "Random Oversampling is a method of selecting minority class samples with replacement(repeated occurences) resulting in higher proportion of minority class samples.\n",
    "\n",
    "\n",
    "![ros](../images/ros.png)\n",
    "\n",
    "\n",
    "We will use the RandomOverSampler() to do this from the package IMBLEARN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling \n",
    "\n",
    "- Initialise a `RandomOverSampler()` object with `random_state=0` and save it to a variable called `'ros'`.\n",
    "\n",
    "- Using `fit_sample()` method of `'ros'`, undersample `'X_train'` and `'y_train'` and store the new samples in variables `'X_ros'` and `'y_ros'`.\n",
    "\n",
    "\n",
    "- Initialise a naive bayes model with `MultinomialNB()` with `random_state=0` and save it to a variable called `'nb'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_ros'` and `'y_ros'` using the `'fit()'` method.\n",
    "\n",
    "- Find out the accuracy score between `X_test` and `'y_test'` using the `'score()'` method and save it in a variable called `'ros_score'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:44:56.448242Z",
     "start_time": "2018-12-24T17:44:55.969008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6016597510373444\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Initialising a random over sampler object\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "#Sampling the train data\n",
    "X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "#Initialsing multinomial naive bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Fitting the sampled train data\n",
    "nb.fit(X_ros,y_ros)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "ros_score=nb.score(X_test,y_test)\n",
    "\n",
    "print(ros_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Linear Kernel SVM\n",
    "\n",
    "We have already covered in detail about Support Vector Machines.\n",
    "\n",
    "To refresh, \n",
    "\n",
    "Support Vector Machines are based on the concept of decision planes that define decision boundaries. \n",
    "In other words, given labeled training data (supervised learning), the algorithm outputs an optimal `hyperplane` which can help categorize new examples. \n",
    "\n",
    "![](../images/kernel_2.png)\n",
    "\n",
    "\n",
    "\n",
    "**Why SVMs work for text classification?**\n",
    "\n",
    "- High Dimensional input space:\n",
    "\n",
    "When dealing with text data, we know we need to deal with many features(>10000 usually). Since SVM(particulary Linear SVM) uses overfitting protection, they have the capability to handle large feature space.\n",
    "\n",
    "\n",
    "- Few irrelevant features:\n",
    "\n",
    "Extension of the above point, during text classification one can't really do a rigourous feature selection. Research has shown that even the features ranked low still contain considerable information. SVM is therefore apt to handle this large amount of feature space in which feature selection or reduction can't be achieved satisfactorily.\n",
    "\n",
    "- Most text categorisation problems are linearly separable\n",
    "\n",
    "Lot of experiments has resulted in the conclusion that text categorisation problems are usually linearly separable, since the concept of SVM is to find such linear separators, SVMS work better than most other models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC\n",
    "\n",
    "Let's use a Linear SVC as our last algorithm, to test the results.\n",
    "\n",
    "- Initialise a support vector model with `SVC()` with `random_state=0`& `kernel=\"linear\"` and save it to a variable called `'svc'`.\n",
    "\n",
    "\n",
    "- Fit the model on the training data `'X_ros'` and `'y_ros'` using the `'fit()'` method.\n",
    "\n",
    "- Find out the accuracy score between `X_test` and `'y_test'` using the `'score()'` method and save it in a variable called `'svc_score'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:46:17.517887Z",
     "start_time": "2018-12-24T17:44:56.450471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648686030428769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Initialising a support vector model with linear kernel\n",
    "svc = SVC(kernel=\"linear\", random_state=0)\n",
    "\n",
    "#Fitting the model on train data\n",
    "svc.fit(X_ros,y_ros)\n",
    "\n",
    "#Finding the accuracy score of the model on test data\n",
    "svc_score=svc.score(X_test,y_test)\n",
    "\n",
    "print(svc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
