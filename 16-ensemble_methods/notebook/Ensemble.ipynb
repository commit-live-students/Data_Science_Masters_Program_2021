{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling and Random Forests\n",
    "\n",
    "Description: \n",
    "Learn about what ensemble is, it's different methods and how it helps improving the solution of machine learning problems.\n",
    "\n",
    "\n",
    "## Overview\n",
    "- Introduction to Ensembling\n",
    "- Soft vs Hard Voting\n",
    "- Aggregation\n",
    "- Stacking\n",
    "- Random Forest\n",
    "- Hyperparameter Tuning\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "-  \n",
    "\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "- Understanding the intuition behind ensemble methods\n",
    "- Working with different types of ensemble methods\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Introduction to Ensembling \n",
    "\n",
    "###    Description: \n",
    "Understand ensembling's intuition and the different types assosciated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Problem Statement\n",
    "\n",
    "***\n",
    "The problem that we will solve is the same one we encountered in the decision tree course(link) related to the banking sector.\n",
    "Just to refresh, the problem statement is as follows: \n",
    "A bank has put out a marketing campaign and wants to know how the campaign is working. Given the features of the client and the marketing campaign, we have to predict whether the customer will subscribe to a term deposit or not. To get a deeper understanding of the problem, you can read more about the problem [here](https://pdfs.semanticscholar.org/a175/aeb08734fd669beaffd3d185a424a6f03b84.pdf). \n",
    " \n",
    "\n",
    "We will be using the dataset with all the 17 features for the purpose of understanding ensemble methods. \n",
    "- `age (numeric)` - age of the bank customer\n",
    "- `job(categorical)`- job of the bank customer\n",
    "- `marital(categorical)`- marital status of the bank customer\n",
    "- `education(categorical)`- Education status of the customer\n",
    "- `default(categorical)` - Whether the customer has credit in default?\n",
    "- `balance (numeric)` - average yearly balance in euros\n",
    "- `housing (categorical)` - Whether the customer has a housing loan?\n",
    "- `loan(categorical)`- Whether the customer has a personal loan?\n",
    "- `contact(categorical)`- contact communication type \n",
    "- `day(numeric)`- last contact date(of the month) of the year\n",
    "- `month(categorical)`- last contact month of year\n",
    "- `day(categorical)`- last contact day of the week (: 'mon','tue','wed','thu','fri')\n",
    "- `duration (numeric)` - last contact duration, in seconds\n",
    "- `campaign (numeric)` - number of contacts performed during this campaign and for this client\n",
    "- `pdays (numeric)`- number of days that passed by after the client was last contacted from a previous campaign \n",
    "- `previous (numeric)`- number of contacts performed before this campaign and for this client (numeric)\n",
    "\n",
    "*** \n",
    "- `Target`: deposit - has the client subscribed a term deposit? (binary- 0: no, 1:yes)\n",
    "\n",
    "\n",
    "We will again try to fit the decision tree model we learned on the data\n",
    "***\n",
    "\n",
    "   \n",
    "**Accuracy score using Decision Tree(Note the overfitting of train data)**\n",
    "\n",
    "```python\n",
    "#Training the model\n",
    "Decision_Tree.fit(data_train, label_train)\n",
    "\n",
    "#Accuracy of the train data\n",
    "Decision_Tree_Score=Decision_Tree.score(data_train,label_train)\n",
    "print(\"Training Score: %.2f \"%Decision_Tree_Score)\n",
    "\n",
    "#Accuracy of the test data\n",
    "Decision_Tree_Score=Decision_Tree.score(data_test,label_test)\n",
    "print(\"Training Score: %.2f \"%Decision_Tree_Score)\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "```python\n",
    "Training Score: 1.00\n",
    "\n",
    "Test Score: 0.76   \n",
    "```\n",
    "***\n",
    "\n",
    "Can we do better than that? \n",
    "\n",
    "We know decision tree is powerful, what if we combine multiple decision tree models together?\n",
    "\n",
    "Let's call this combination model as ensemble model\n",
    "\n",
    "***\n",
    "\n",
    "**Accuracy score using the Ensemble model** \n",
    "\n",
    "```python\n",
    "Ensemble_Model.fit(data_train, label_train)\n",
    "\n",
    "#Accuracy of the train data\n",
    "Ensemble_Model_Score=Ensemble_Model.score(data_train,label_train)\n",
    "print(\"Training Score: %.2f \"%Ensemble_Model_Score)\n",
    "\n",
    "#Accuracy of the test data\n",
    "Ensemble_Model_Score=Ensemble_Model.score(data_test,label_test)\n",
    "print(\"Training Score: %.2f \"%Ensemble_Model_Score)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Training Score: 0.83\n",
    "\n",
    "Test Score: 0.82   \n",
    "\n",
    "```\n",
    "***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Wisdom of Crowd\n",
    "\n",
    "\n",
    "It is the idea that when it comes to problem solving and decision making, collective intelligence of many often surpasses the intelligence of a single expert.\n",
    "\n",
    " ![Wisdom of Crowds](..\\images\\wisdom_of_crowds.jpg)\n",
    " \n",
    " \n",
    " \n",
    "For eg: Suppose you decide you want to go to 'Rome' for your vacation. However, you are not sure if it is a good place to visit during Summer. So you ask a bunch of people\n",
    "\n",
    "1. A travel guide, whose opinions about travel destination are 70% times similar to yours.\n",
    "2. A YouTube trip vlogger, who is 80% times similar to your opinions about a destination.\n",
    "3. A close friend of yours, who is 60% of times similar to your opiniions. \n",
    "\n",
    "Though individually each one would may have some sort of bias(For eg: Your friend said no cause of his aversion towards forts in Rome)but when taken together, the probabilty of them being wrong simultaneously is equal to-\n",
    ">* P = (1 - 0.7) x (1 - 0.8) x (1 - 0.6)\n",
    ">* P = 0.024\n",
    "\n",
    "Which means that there is 97.6% chance that their opinion will be good (given their opinions are independent from each other).\n",
    "\n",
    "Ensemble works on similar principles.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Ensemble modeling is a machine learning technique of combining multiple machine learning models to produce one optimal model . \n",
    "Though there exists many different techniques of it, at their core they all employ the same two methods:\n",
    "\n",
    "- Produce a cohort of predictions using simple ML algorithms.\n",
    "- Combine the predictions into one \"aggregated\" model.\n",
    "\n",
    "### Why ensemble modeling?\n",
    "In real world scenarios, generalizing on a dataset by a single model can be challenging. Some models will be able to capture one aspect of the data well while others will do well in capturing something else. Ensemble modeling provides us a with family of techniques that help reduce errors and make predictions where the final evaluation metrics(For eg: Accuracy) are better than they are for each of the individual models.\n",
    "\n",
    "![ensemble_method](..\\images\\ensemble_cat.jpg)\n",
    "\n",
    "\n",
    "Let's further explore this technique of Ensembling using a mathematical thought process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Strong vs Weak Learner\n",
    "\n",
    "\n",
    "\n",
    "**Condorcet’s Jury Theorem**\n",
    "\n",
    "\n",
    " ![Jury](..\\images\\jury.jpg)\n",
    " \n",
    " \n",
    "Let's say a jury of voters are needed to make a decision regarding a binary outcome (for example to convict a defendant or not).\n",
    "\n",
    "If each voter has a probability p of being correct and the probability of a majority of voters being correct is L, then **L > p if p > 0.5** if the voters as independent from each other. Interestingly, **L approaches 1 as the number of voters approaches infinity**.\n",
    "\n",
    "\n",
    "In human language, p > 0.5 means that the individual judgments (votes) are at least a little better than random chance.\n",
    "\n",
    "Now, let's take this analogy to the world of ML:\n",
    "\n",
    "* Verdict ~> classification prediction\n",
    "* Jury members ~> ML models\n",
    "* votes ~> individual predictions\n",
    "\n",
    "This means that employing multiple ML models should improve the performance according to the Condorcet's theorem( and it does!)\n",
    "\n",
    "\n",
    "We only need a large number of learners, whose predictive power is just slightly better than random chance (tossing a coin in case of binary classification problem!) for ensembling to work.Such learners have a special name --\"weak learners\".\n",
    "\n",
    "Formally, they are defined as:\n",
    "\n",
    "* **Weak Learner:**\n",
    "Given a labeled dataset, a Weak Learner produces a classifier which is at least a little more accurate than random classification.\n",
    "\n",
    "\n",
    "\n",
    "* **Strong Learner:**\n",
    "We call a machine learning model a Strong Learner which, given a labeled dataset, can produce results arbitrarily well-correlated with the true classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Different techniques of Ensembling\n",
    "\n",
    "Following are the different techniques, ensemble modeling is broadly divided into:\n",
    "\n",
    "\n",
    "#### 1. Voting/Aggregating \n",
    "\n",
    "`This technique involves building multiple models(usually of differing types) and the predictions which we get after averaging(regression) or voting(classification) the results of the models are used as the final prediction.`\n",
    "\n",
    "\n",
    "![Voting](..\\images\\VA.jpg)\n",
    " \n",
    "\n",
    "#### 2. Stacking\n",
    "\n",
    "`This technique involves combining multiple classification models via a meta-classifier i.e. instead of using trivial functions to aggregate the predictions , a model is trained to perform this aggregation.`\n",
    "\n",
    "![Stacking](..\\images\\stacking.jpg)\n",
    "\n",
    "\n",
    "#### 3. Boosting\n",
    "\n",
    "`This technique involves a sequential process, where each subsequent model attempts to correct the errors of the previous model. More weight is given to examples that were misclassified by earlier rounds and then the final prediction is produced by combining the results using a weighted average approach.`\n",
    "\n",
    "![Boosting](..\\images\\Boostingg.jpg)\n",
    "\n",
    "\n",
    "For image reference-http://manish2020.blogspot.com/\n",
    "\n",
    "\n",
    "We will go through each of them(except boosting) in the susbsequent chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Aggregation\n",
    "\n",
    "###    Description: \n",
    "Understand in depth about the aggregation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Naive Aggregation\n",
    "\n",
    "Suppose you want to watch a movie of your favourite actor this weekend. His last two movies had been disappointing, so you decide to watch the movie based upon the ratings given by your three friends. \n",
    "The most obvious and intuitive way then would be to average out all the three ratings and make your decision.\n",
    "\n",
    "Similarly the most intuitive way to combine models is averaging out their indvidual predictions. \n",
    "\n",
    "**Naive aggregation** works by aggregating the final output through averaging (regression) or voting (classification).\n",
    "It works best with algorithms which learn very differently from each other, thereby complementing each others' decisions.\n",
    "\n",
    "##### Soft Voting vs Hard Voting\n",
    "***\n",
    "Since, every classification algorithm first calculates the probabilities of each outcome, and then produces the prediction, the aggregation could be done either on calculated probabilities, or final predictions.\n",
    "\n",
    "* In **hard voting**, the voting classifier takes majority of its base learners’ predictions\n",
    "* In **soft voting**, the voting classifier takes into account the probability values by its base learners \n",
    "\n",
    "In general, soft voting has been observed to perform better than hard voting.\n",
    "\n",
    "\n",
    "##### Python Implementation of Voting\n",
    "\n",
    "For this we will use a subset of our original dataset containing only 3000 datapoints.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Four random models intitialised\n",
    "log_clf_1 = LogisticRegression(random_state=0)\n",
    "log_clf_2 = LogisticRegression(random_state=42)\n",
    "decision_clf1 = DecisionTreeClassifier(criterion = 'entropy',random_state=0)\n",
    "decision_clf2 = DecisionTreeClassifier(criterion = 'entropy', random_state=42)\n",
    "\n",
    "\n",
    "#Creating a list of models\n",
    "Model_List=[('Logistic Regression 1', log_clf_1),\n",
    "            ('Logistic Regression 2', log_clf_2),\n",
    "            ('Decision Tree 1', decision_clf1),\n",
    "            ('Decision Tree 2', decision_clf2)]\n",
    "\n",
    "\n",
    "#Features\n",
    "X= bank_sample.drop(['deposit'],1)\n",
    "\n",
    "#Target variable\n",
    "y=bank_sample['deposit'].copy()\n",
    "\n",
    "\n",
    "#Splitting into train and test dataset\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "#Initialising hard voting model\n",
    "voting_clf_hard = VotingClassifier(estimators = Model_List,\n",
    "                                   voting = 'hard')\n",
    "\n",
    "#Fitting the data\n",
    "voting_clf_hard.fit(Data_train, label_train)\n",
    "\n",
    "#Scoring the model for train\n",
    "hard_voting_score=voting_clf_hard.score(Data_train,label_train)\n",
    "print(\"Hard Voting Train Accuracy:%.2f\"%hard_voting_score)\n",
    "\n",
    "\n",
    "#Scoring the model for test\n",
    "hard_voting_score=voting_clf_hard.score(Data_test,label_test)\n",
    "print(\"Hard Voting Test Accuracy:%.2f\"%hard_voting_score)\n",
    "\n",
    "#Initialising soft voting model\n",
    "voting_clf_soft = VotingClassifier(estimators = Model_List,voting = 'soft')\n",
    "\n",
    "\n",
    "#Fitting the data\n",
    "voting_clf_soft.fit(Data_train, label_train)\n",
    "\n",
    "#Scoring the model for train\n",
    "soft_voting_score= voting_clf_soft.score(Data_train,label_train)\n",
    "print(\"Soft Voting Train Accuracy: %.2f\"%soft_voting_score)\n",
    "\n",
    "#Scoring the model for test\n",
    "soft_voting_score= voting_clf_soft.score(Data_test,label_test)\n",
    "print(\"Soft Voting Test Accuracy: %.2f\"%soft_voting_score)\n",
    "\n",
    "#Solution ends\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "\n",
    "Hard Voting Train Accuracy:0.88\n",
    "\n",
    "Hard Voting Test Accuracy:0.75\n",
    "    \n",
    "Soft Voting Train Accuracy: 1.00\n",
    "\n",
    "Soft Voting Test Accuracy: 0.75\n",
    "```\n",
    "\n",
    "Let's now apply Soft Voting and Hard Voting on the complete bank dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 - Using voting method for prediction\n",
    "\n",
    "In this task, you will apply voting method on different ML models to predict target of our `bank problem`.\n",
    "\n",
    "***\n",
    "- Load the dataset from the path using the `read_csv()` method from pandas and store it in a variable called data \n",
    "\n",
    "- Look at the first 10 rows of the data using the `head()` method. [For you to see the dataset features] \n",
    "\n",
    "- Store all the features of `'data'` in  a variable called `X`\n",
    "\n",
    "- Store the target variable (`deposit`) of `'data'` in a variable called `y`\n",
    "\n",
    "- Split `'X'` and `'y'` into `X_train,X_test,y_train,y_test` using `train_test_split()` function. Use `test_size = 0.3` and `random_state = 0`\n",
    "\n",
    "- Four different ML models for ensmbling has already been defined in the notebook for you\n",
    "\n",
    "- Use the `VotingClassifier()` from sklearn to initialize a voting classifier object Pass the `'Model_List'` as input to the `estimators` parameter and `'hard'` to the `voting` parameter while initializing the object. Save the object in a variable `'voting_clf_hard'`.\n",
    "\n",
    "- Use the `fit()` method of the `'voting_clf_hard'` to train the model on the `'X_train'` and `'y_train'`. \n",
    "\n",
    "- Use the `score()` method of the `voting_clf_hard` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store it in a variable called `'hard_voting_score'`. \n",
    "\n",
    "- Repeat the same steps for soft voting.\n",
    "\n",
    "- Use the `VotingClassifier()` from sklearn to initialize a voting classifier object Pass the `'Model_List'` as input to the `estimators` parameter and `'soft'` to the `voting` parameter while initializing the object. Save the object in a variable `'voting_clf_soft'`.\n",
    "\n",
    "- Use the `fit()` method of the `'voting_clf_soft'` to train the model on the `'X_train'` and `'y_train'`. \n",
    "\n",
    "- Use the `score()` method of the `voting_clf_soft` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store it in a variable called `'soft_voting_score'`. \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "After the task, compare both the accuracy scores.\n",
    "(Additionaly, you could also try to use different combinations of the given machine learning models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting Test Accuracy:0.77\n",
      "Soft Voting Test Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Different models initialised\n",
    "log_clf_1 = LogisticRegression(random_state=0)\n",
    "log_clf_2 = LogisticRegression(random_state=42)\n",
    "decision_clf1 = DecisionTreeClassifier(criterion = 'entropy',random_state=0)\n",
    "decision_clf2 = DecisionTreeClassifier(criterion = 'entropy', random_state=42)\n",
    "\n",
    "\n",
    "#Creation of list of models\n",
    "Model_List=[('Logistic Regression 1', log_clf_1),\n",
    "            ('Logistic Regression 2', log_clf_2),\n",
    "            ('Decision Tree 1', decision_clf1),\n",
    "            ('Decision Tree 2', decision_clf2)]\n",
    "\n",
    "#Solution begins\n",
    "\n",
    "\n",
    "path='../data/bank_data_new.csv'\n",
    "data=pd.read_csv(path)\n",
    "\n",
    "#Features\n",
    "X= data.drop(['deposit'],1)\n",
    "\n",
    "#Target variable\n",
    "y=data['deposit'].copy()\n",
    "\n",
    "\n",
    "#Splitting into train and test dataset\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "#Initialising hard voting model\n",
    "voting_clf_hard = VotingClassifier(estimators = Model_List,\n",
    "                                   voting = 'hard')\n",
    "\n",
    "#Fitting the data\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "\n",
    "#Scoring the model for test\n",
    "hard_voting_score=voting_clf_hard.score(X_test,y_test)\n",
    "print(\"Hard Voting Test Accuracy:%.2f\"%hard_voting_score)\n",
    "\n",
    "#Initialising soft voting model\n",
    "voting_clf_soft = VotingClassifier(estimators = Model_List,voting = 'soft')\n",
    "\n",
    "\n",
    "#Fitting the data\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "#Scoring the model for test\n",
    "soft_voting_score= voting_clf_soft.score(X_test,y_test)\n",
    "print(\"Soft Voting Test Accuracy: %.2f\"%soft_voting_score)\n",
    "\n",
    "#Solution ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bootstrap Aggregation(Bagging)\n",
    "\n",
    "Continuing with the movie dilemma of your favourite actor. After getting your friends' opinions, you are still not satisfied and think to yourself, \n",
    "\n",
    "**What could better than the wisdom of crowds?**\n",
    "\n",
    "**Ans. Wisdom of diverse experts!**.\n",
    "\n",
    "A classic example of it, is the **minister cabinet** of the kings in the older times, where each minister used to be an expert of a particular area and the king would ask for opinions from them before taking any major decisions.\n",
    "\n",
    "So instead of going with the opinion of your friends, you decide to see the review of some respected critics. Now instead of getting a general opinion, you have taken the review of movie critics who have unrelated and independent views about the movie.  \n",
    "\n",
    "**A similar approach is used in bagging. Each base learner is trained on different sample of data making each learner, a `specialist base learner`.**\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Bagging which **B**ootstrap **AGG**regat**ING** is usually called, is an approach to ensemble learning where given a training set,  multiple different training sets (called bootstrap samples) are created, by sampling with replacement from the original dataset. Then, for each bootstrap sample, a model is built.The individual predictions are then aggregated to form a final prediction.\n",
    "\n",
    "Unlike naive aggregator, bagging uses a single type of base learner\n",
    "\n",
    "![Bagging](..\\images\\Bagging.jpg)\n",
    "\n",
    "\n",
    "#### Bias-Variance trade off \n",
    "***\n",
    "\n",
    "\n",
    "To better understand model predictions, it’s important to understand the prediction errors:\n",
    "\n",
    "1. Bias \n",
    "\n",
    "2. Variance \n",
    "\n",
    "Consider the following:\n",
    "\n",
    "Assume a dataset with features 'X' and target variable 'y' and you create a model 'F(X)' for predicting 'y'.\n",
    "\n",
    "\n",
    "The expected squared error for a point x, will be then defined as :\n",
    "\n",
    "$Error(x)=E[(y- F(x))^2]$    `#E(x)=avg(x)`\n",
    "\n",
    "$Error(X)$ can be further broken down into the following:\n",
    "\n",
    "$Error(x)=(E[F(x)]- y)^2  + E[(F(x)- E[F(x)])^2] + \\alpha$\n",
    "\n",
    "Which can also be written as:\n",
    "\n",
    "$E(X)=Bias^2  + Variance + Noise$\n",
    "\n",
    " \n",
    "**Error due to Bias:**\n",
    "\n",
    "The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. \n",
    "\n",
    "**Error due to Variance:**\n",
    "\n",
    "The error due to variance is taken as the variability of a model prediction for a given data point.\n",
    "\n",
    "\n",
    "Since both are errors, you would ideally want a model having both low bias and low variance\n",
    "\n",
    "However to achieve that is difficult and there is always a tradeoff between a model’s ability to minimize bias and variance. \n",
    "\n",
    "Let's understand that with an example:\n",
    "\n",
    "Consider the following data distibution\n",
    "\n",
    "\n",
    "\n",
    "![](../images/bv_1.jpg)\n",
    "\n",
    "Following will be the model if we try to `reduce bias error`:\n",
    "\n",
    "![](../images/bv_2.jpg)\n",
    "\n",
    "Two things you can observe:\n",
    "\n",
    "* The model has overfit the data(The model has become super complex and has fitted even noise)\n",
    "\n",
    "* While reducing bias, the variance has increased\n",
    "\n",
    "\n",
    "\n",
    "Similarly, following will be the model if we try to `reduce variance error`:\n",
    "\n",
    "![](../images/bv_3.jpg)\n",
    "\n",
    "Two things you can observe:\n",
    "\n",
    "* The model has underfit the data(The model is too simple and has fitted the data too poorly)\n",
    "\n",
    "* While reducing variance, the bias has increased\n",
    "\n",
    "\n",
    "Your ideal model therefore becomes one having the perfect-bias variance tradeoff\n",
    "\n",
    "![](../images/bv_4.jpg)\n",
    "\n",
    "\n",
    "As clear from the example, since a model can't be both less complex and more complex at the same time, there is no escaping the relationship between bias and variance in machine learning.\n",
    "\n",
    "\n",
    "* Increasing in bias = decrease in variance. \n",
    "\n",
    "* Increasing in variance = decrease in bias.\n",
    "\n",
    "Following is a graph explaining the error contributed by the bias and variance\n",
    "\n",
    "![](../images/bv_5.jpg)\n",
    "\n",
    "\n",
    "Another way to look at it is using the following diagram:\n",
    "\n",
    "![](../images/bias_variance_tradeoff_2.png)\n",
    "\n",
    "Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the target, our predictions get worse and worse. \n",
    "\n",
    "The optimum model of course is the one with low bias and low variance.\n",
    "\n",
    "High bias(low variance) algorithm train models that are consistent but inaccurate on average.\n",
    "\n",
    "High variance(low bias) algorithm train models that are accurate but inconsistent on average.\n",
    "\n",
    "Having both high variance and high bias results in the least optimum model which is both inconsistent and inaccurate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Bias-Variance Tradeoff in Bagging**\n",
    "\n",
    "In bagging, because of bootstrapping, each individual predictor has **a higher bias** than if it were trained on the original training set. However, a large number of such biases will get reduced when aggregated, hence the bias of the resulting bagging is only slightly higher than a comparable single predictor strong learner. \n",
    "\n",
    "\n",
    "\n",
    "At the same time, because bagging provides a way to **reduce overfitting** owing to less dependence on one particular subset of training data, the **variance of resulting strong learner reduces** significantly.\n",
    "\n",
    "\n",
    "Generally, the net result is that the ensemble has a **similar bias** but a **lower variance** than a single predictor trained on the original training set.\n",
    "\n",
    "##### Python Implementation of Bagging\n",
    "\n",
    "For this we will use the same subset of our original dataset containing only 3000 datapoints.\n",
    "\n",
    "```python\n",
    "\n",
    "#Features\n",
    "X= bank_sample.drop(['deposit'],1)\n",
    "\n",
    "#Target variable\n",
    "y=bank_sample['deposit'].copy()\n",
    "\n",
    "\n",
    "#Splitting into train and test dataset\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Initialising bagging with appropriate parameters\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100, random_state=0)\n",
    "\n",
    "#Fitting the data\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "#Scoring the model for train data\n",
    "score_bc_dt = bagging_clf.score(X_train, y_train)\n",
    "print(\"Training score: %.2f \" % score_bc_dt)\n",
    "\n",
    "\n",
    "#Scoring the model for test data\n",
    "score_bc_dt = bagging_clf.score(X_test, y_test)\n",
    "print(\"Testing score: %.2f \" % score_bc_dt)\n",
    "\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```python\n",
    "Training score: 0.82\n",
    "    \n",
    "Testing score: 0.80\n",
    "```\n",
    "\n",
    "Let us now apply Bagging to solve the problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 - Using bagging for prediction\n",
    "\n",
    "In this task, you will apply Bagging of decision trees to predict target.\n",
    "\n",
    "\n",
    "***\n",
    "- Use the `BaggingClassifier()` from sklearn to initialize a bagging classifier object. Pass the parameter `base_estimator`= DecisionTreeClassifier, `n_estimators`=100 ,  `max_samples`=100 and `random_state`=0, while initializing the object. Store the object in the variable `'bagging_clf'`\n",
    "\n",
    "\n",
    "- Use the `fit()` method of the bagging classifier object `'bagging_clf'` on `'X_train'` and `'y_train'` to train the models on the training data. \n",
    "\n",
    "\n",
    "- Use the `score()` method of the bagging classifier object `'bagging_clf'` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store the score in a variable called `'score_bagging'`\n",
    "***\n",
    "\n",
    "After the task, compare the accuracy score with the previous voting method.\n",
    "Has it improved? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.81 \n"
     ]
    }
   ],
   "source": [
    "# Fitting bagging classifier with Decision Tree\n",
    "# path='../data/bank_data_new.csv'\n",
    "# data=pd.read_csv(path)\n",
    "\n",
    "# bank_sample=data.sample(n=3000,random_state=0)\n",
    "\n",
    "# #Features\n",
    "# X= bank_sample.drop(['deposit'],1)\n",
    "\n",
    "# #Target variable\n",
    "# y=bank_sample['deposit'].copy()\n",
    "\n",
    "\n",
    "# #Splitting into train and test dataset\n",
    "# X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Initialising bagging with appropriate parameters\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100, random_state=0)\n",
    "\n",
    "#Fitting the data\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Scoring the model for test data\n",
    "score_bagging = bagging_clf.score(X_test, y_test)\n",
    "print(\"Score: %.2f \" % score_bagging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.Pasting\n",
    "\n",
    "In bagging we tried to create samples through resampling with replacement, in the same way, we can create samples resampling **without replacement** for each base learner. Ensemble on such samples is known as **Pasting**.\n",
    "\n",
    "Replacement introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting. \n",
    "\n",
    "But in pasting the predictors end up being more correlated so the ensemble’s variance is increased. \n",
    "\n",
    "**Image**\n",
    "\n",
    "**Overall, bagging often results in better models than pasting** \n",
    "\n",
    "However, given spare time and CPU power it is worth using cross- validation to evaluate both bagging and pasting and select the one that works best.\n",
    "\n",
    "Python implementation of `pasting` is same as `bagging` with an added parameter of changing `bootstrap=False`\n",
    "\n",
    "```python\n",
    "\n",
    "BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100,bootstrap=False, random_state=0)\n",
    "```\n",
    "Let us try and implement Pasting on our dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Using pasting for prediction\n",
    "\n",
    "In this task, you will apply Pasting to predict target.\n",
    "\n",
    "***\n",
    "- Use the `BaggingClassifier()` from sklearn to initialize a bagging classifier object. Pass the parameter `base_estimator`= DecisionTreeClassifier, `n_estimators`=100 ,  `max_samples`=100, `bootstrap`=False and `random_state`=0, while initializing the object. Store the object in the variable `'pasting_clf'`.\n",
    "\n",
    "- Use the `fit()` method of the bagging classifier object `'pasting_clf'` on `'X_train'` and `'y_train'` to train the models on the training data. \n",
    "\n",
    "- Use the `score()` method of the bagging classifier object `'pasting_clf'` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store the score in a variable called `'score_pasting'`\n",
    "***\n",
    "\n",
    "After the task, compare the accuracy score with the bagging method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasting score: 0.81 \n"
     ]
    }
   ],
   "source": [
    "# Fitting pasting with Decision Tree\n",
    "pasting_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100,bootstrap=False, random_state=0)\n",
    "\n",
    "pasting_clf.fit(X_train, y_train)\n",
    "\n",
    "score_pasting = pasting_clf.score(X_test, y_test)\n",
    "print(\"Pasting score: %.2f \" % score_pasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Random Forest.\n",
    "\n",
    "Just when you were about to decide whether you want to watch a movie, one of your friends asked you to go to a party with him. \n",
    "Now you realise in order to make your decision you need to be absolutely sure if the movie is good, \n",
    "otherwise you will regret not going to party. You start thinking, \n",
    "\n",
    "**How can you improve upon the wisdom of diverse experts?**\n",
    "\n",
    "**Ans. Experts whose area of expertise is more inclined towards and suitable for solving that particular problem**\n",
    "\n",
    "So keeping that in mind, from the many movie critics, you select the following three:  \n",
    "* Critic 1: A respected critic whose opinion about movies usually resonates with your taste in movies. \n",
    "\n",
    "* Critic 2: A youtube critic who is a huge fan of the actor's movies\n",
    "\n",
    "* Critic 3: A filmmaker turned critic who specialises in reviewing the particular genre the film is of.\n",
    "\n",
    "\n",
    "Random forests work similarly by taking only those features that work best to find the optimum solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "Random forest is an ensemble method of bagging multiple decision trees. The fundamental difference is that in Random Forests, along with bootstrap sampling, only a subset of features are selected at random out of the total features\n",
    "\n",
    "    \n",
    "           \n",
    "![RandomVBagging](..\\images\\BvR.jpg)\n",
    "\n",
    "\n",
    "Random forest is one of the most popular ensemble algorithms(or for that matter, one of the most popular ML algorithms) owing to it's \n",
    "- inherent feature selection \n",
    "- simplicity to train \n",
    "- and versatilite problem solving(it can be used for classification, regression, cluster analysis..).\n",
    "\n",
    "Although there exists models that can beat Random Forests for a given dataset (usually boosting or neural network), it’s never by a big margin. Additionaly it also takes much longer to build those models than it takes to build the Random Forest, making them excellent benchmark models.\n",
    "\n",
    "\n",
    "#### Bias variance trade off in Random Forests\n",
    "\n",
    "Random forests results in a greater tree diversity, which trades a **higher bias**(Owing to features getting subsetted) for a **lower variance**, generally yielding an **overall better model**.\n",
    "\n",
    "\n",
    "##### Python Implementation of Random Forests\n",
    "\n",
    "For this we will use the same subset of our original dataset containing only 3000 datapoints.\n",
    "\n",
    "```python\n",
    "\n",
    "#Features\n",
    "X= bank_sample.drop(['deposit'],1)\n",
    "\n",
    "#Target variable\n",
    "y=bank_sample['deposit'].copy()\n",
    "\n",
    "\n",
    "#Splitting into train and test dataset\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "#Initialising Random Forest model\n",
    "rf_clf=RandomForestClassifier(n_estimators=100,n_jobs=100,random_state=0, min_samples_leaf=100)\n",
    "\n",
    "#Fitting on data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "#Scoring the model on train data\n",
    "score_rf=rf_clf.score(X_train, y_train)\n",
    "print(\"Training score: %.2f \" % score_rf)\n",
    "\n",
    "#Scoring the model on test_data\n",
    "score_rf=rf_clf.score(X_test, y_test)\n",
    "print(\"Testing score: %.2f \" % score_rf)\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "```python\n",
    "Training score: 0.80 \n",
    "\n",
    "Testing score: 0.79 \n",
    "```\n",
    "\n",
    "Let us now apply Random Forests to our complete bank statement problem dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 - Using Random Forest for prediction\n",
    "\n",
    "In this task, you will apply Random Forest to predict the target.\n",
    "\n",
    "***\n",
    "- Use the `RandomForestClassifier()` from sklearn to initialize a random forest classifier object. Pass the parameter `n_estimators`=100,`n_jobs`=100, `min_samples_leaf`=100 and `random_state`=0, while initializing the object. Store the object in the variable `'rf_clf'`.\n",
    "\n",
    "- Use the `fit()` method of the bagging classifier object `'rf_clf'` on `'X_train'` and `'y_train'` to train the models on the training data. \n",
    "\n",
    "- Use the `score()` method of the bagging classifier object `'rf_clf'` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store the score in a variable called `'score_rf'`\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "After the task, compare the accuracy score with the previous voting method.\n",
    "Has it improved? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.82 \n"
     ]
    }
   ],
   "source": [
    "#Initialising Random Forest model\n",
    "rf_clf=RandomForestClassifier(n_estimators=100,n_jobs=100,random_state=0, min_samples_leaf=100)\n",
    "\n",
    "#Fitting on data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "#Scoring the model on test_data\n",
    "score_rf=rf_clf.score(X_test, y_test)\n",
    "print(\"Testing score: %.2f \" % score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Hyper parameter tuning\n",
    "\n",
    "###    Description: \n",
    "USe hyperparameter tuning to improve the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defintion\n",
    "\n",
    "\n",
    "Random Forest is indeed impressive. We were able to increase the score of the model by almost 6%(in comparision to the score of our decision tree).\n",
    "\n",
    "**Note**: Random forests was the ensemble method used as the example while introducing the ensmble method concept in the first chapter.\n",
    "\n",
    "The Random Forest object that we used in the previous task was the default one, more specifically the one below:\n",
    "\n",
    "`RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=100, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=100,\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)`\n",
    "            \n",
    "You can learn what each parameter in the model means from the beautifully maintained documentation of [Random Forest by sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).            \n",
    "\n",
    "Now the question that we are interested in is this,\n",
    "\n",
    "*If the above given parameters are the reason for model to behave a particular way, coudn't we change them to a desired combination to achieve the optimum model?*\n",
    "\n",
    "Enter **Hyperparameter Tuning**\n",
    "\n",
    "\n",
    "You already learned about hyperparameter tuning before\n",
    "\n",
    "Just to refresh, hyperparameters are the parameters which define the architecture of model and the process of searching for the ideal hyperparameters for model optimization is referred to as hyperparameter tuning.\n",
    "\n",
    "\n",
    "For this task we will be using two kinds of hyperparameter tuning method:\n",
    "1. Grid Search\n",
    "2. Randomised Search \n",
    "\n",
    "\n",
    "### 3.2 Grid Search\n",
    "\n",
    "In Grid search, exhaustive search over specified parameter values for an estimator is done.\n",
    "\n",
    "Let us try to do Grid Search on Random Forest and see if the performance of the model is improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5 - Using Grid Search for Random Forest\n",
    "\n",
    "In this task, you will apply Grid Search on Random Forest to hypertune parameters.\n",
    "\n",
    "***\n",
    "- The parameter grid for hypertuning is already given.\n",
    "\n",
    "- Create a `RandomForestClassifer()` object with `random_state=0` and store it in a variable called `'clf'`\n",
    "\n",
    "- Use the `GridSearchCV()` from sklearn to initialize a grid search object. Pass the parameters `estimator=clf`,`param_grid =parameter grid`  while initializing the object. Store the object in a variable called `'grid_search'`\n",
    "\n",
    "- Use the `fit()` method of the bagging classifier object `'grid_search'` on `'X_train'` and `'y_train'` to train the models on the training data. \n",
    "\n",
    "- Use the `score()` method of the bagging classifier object `'grid_search'` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store the score in a variable called `'score_gs'`\n",
    "***\n",
    "\n",
    "After the task, compare the accuracy score with the Random Forest method.\n",
    "Has it improved?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.84 \n"
     ]
    }
   ],
   "source": [
    "#Parameter grid\n",
    "parameter_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\"\"\"Solution begin \"\"\"\n",
    "\n",
    "clf= RandomForestClassifier(random_state=0)\n",
    "grid_search = GridSearchCV(clf, param_grid=parameter_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# GS_score=grid_search.score(Data_train,label_train)\n",
    "# print(\"Training score: %.2f \" % GS_score)\n",
    "\n",
    "score_gs=grid_search.score(X_test,y_test)\n",
    "print(\"Testing score: %.2f \" % score_gs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Search\n",
    "\n",
    "In this method of hyper parameter tuning, Randomized search on hyper parameters is done.\n",
    "\n",
    "Let us try to do Randomised Search on Random Forest hyperparameter, see if the performance of the model is improved and compare it with the Grid Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6 - Using Randomized Search for Random Forest\n",
    "\n",
    "In this task, you will apply Randomized Search on Random Forest to hypertune parameters.\n",
    "\n",
    "***\n",
    "- The parameter grid for hypertuning is already given (Note: It is the same as the one given for Grid Search).\n",
    "- Create a `RandomForestClassifer()` object with `random_state=0` and store it in a variable called `'clf'`\n",
    "\n",
    "- Use `RandomizedSearchCV()` from sklearn to initialize a grid search object. Pass the parameters `estimator=clf`,`param_grid =parameter grid`,`n_iter=20`   while initializing the object. Store the object in a variable called `'random_search'`\n",
    "\n",
    "- Use the `fit()` method of the bagging classifier object `'random_search'` on `'X_train'` and `'y_train'` to train the models on the training data. \n",
    "\n",
    "- Use the `score()` method of the bagging classifier object `'random_search'` on `'X_test'` and `'y_test'` to find out the accuracy of the test data and store the score in a variable called `'score_gs'`\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.84 \n"
     ]
    }
   ],
   "source": [
    "#Parameter grid\n",
    "parameter_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\"\"\"Solution begin\"\"\"\n",
    "\n",
    "clf= RandomForestClassifier(random_state=0)\n",
    "\n",
    "# n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=parameter_grid,\n",
    "                                   n_iter=20)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# score_rs=random_search.score(Data_train,label_train)\n",
    "# print(\"Training score: %.2f \" % score_rs)\n",
    "\n",
    "score_rs=random_search.score(X_test,y_test)\n",
    "print(\"Testing score: %.2f \" % score_rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically grid search is the better option, compared to the randomized one because with grid search there is a guarantee of finding the most optimum model. However,it has been observed that randomized search almost always produces near optimum results, that too at a lesser time compared to grid search.\n",
    "That is why random search combined with clever heuristics, is often used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Stacking\n",
    "\n",
    "###   Description: \n",
    "Learn about stacking and it's implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been using naive methods of averaging and voting to combine the predictions, therby implicitly assigning same weights to the predictions made by all the base learners \n",
    "\n",
    "However, it might be possible that some base learners might be better at predicting than the others. So, a better aggregation scheme could be to assign some kind of weights to the predictions made by base learners.\n",
    "\n",
    "We could do it manually but since we have been learning machine learning to predict things anyway,why not use a machine learning model to do it. \n",
    "\n",
    "That's exactly what stacking helps us achieve\n",
    "\n",
    "\n",
    "### Definition\n",
    "\n",
    "Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier(fancy name for a 'classifier of classifiers').\n",
    "It is based on a simple idea: instead of using trivial functions to aggregate the predictions of all predictors in an ensemble, we train a model to perform this aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Steps of Stacking\n",
    "***\n",
    "* First, the training set is split in two subsets. \n",
    "\n",
    "![](..\\images\\Stacking_1.jpg)\n",
    "* The first subset is used to train the 'n' models in the first layer\n",
    "\n",
    "![](..\\images\\Stacking_2.jpg)\n",
    "* Next, the first layer models are used to make predictions on the second (held-out) subset\n",
    "\n",
    "![](..\\images\\Stacking_3.jpg)\n",
    "* The predictions of the models are then stored along with the actual predictions as a new training set.\n",
    "\n",
    "![](..\\images\\Stacking_4.jpg)\n",
    "\n",
    "* The meta-classifier is then trained on this new training set, so it learns to predict the target value given the first layer’s models.\n",
    "\n",
    "![](..\\images\\Stacking_5.jpg)\n",
    "\n",
    "\n",
    "Let's now apply stacking on our bank problem dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7 - Using Stacking\n",
    "\n",
    "In this task, you will apply Stacking to predict the target.\n",
    "\n",
    "***\n",
    "- First layer machine learning models and the meta classifier are already defined for you.\n",
    "- Use the `Stacking()` from  mlxtend to initialize a stacking classifier object. Pass the `'classifier_list'` to parameter `classifiers` and `'m_classifier'` as `meta_classifier`parameter , while initializing the object.\n",
    "- Use the `fit()` method of the stacking classifier object to train the models on the training data. \n",
    "- Use the `score()` method of the stacking classifier object to find out the accuracy of the test data.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.78 \n"
     ]
    }
   ],
   "source": [
    "classifier1 = DecisionTreeClassifier(random_state=0)\n",
    "classifier2= DecisionTreeClassifier(random_state=1)\n",
    "classifier3 = DecisionTreeClassifier(random_state=2)\n",
    "classifier4= DecisionTreeClassifier(random_state=3)\n",
    "classifier_list=[classifier1,classifier2,classifier3,classifier4]\n",
    "\n",
    "m_classifier=LogisticRegression(random_state=0)\n",
    "\n",
    "\"\"\"Solution begin\"\"\"\n",
    "\n",
    "sclf = StackingClassifier(classifiers=classifier_list, \n",
    "                          meta_classifier=m_classifier)\n",
    "\n",
    "sclf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# s_score=sclf.score(X_train,y_train)\n",
    "# print(\"Training score: %.2f \" % s_score)\n",
    "\n",
    "s_score=sclf.score(X_test,y_test)\n",
    "print(\"Test score: %.2f \" % s_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though for this particular dataset, the stacking method was not as effective as bagging or random forest, nonetheless stacking is a powerful ensemble technique worth trying to convert weak learners to strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: \n",
    "\n",
    "Throughout this course, we have tried to understand the different ensemble techinqiues and their practical implementations.\n",
    "While you take time to imbibe all that was taught, find below a quick summary of the different techniques learnt.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Summary](..\\images\\Summary.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "***\n",
    "\n",
    "1. Ensemble methods is loosely based on the concept of ?\n",
    "\n",
    "    a. 'Wisdom of crowds'\n",
    "    \n",
    "    b. 'Survival of the Fittest'\n",
    "\n",
    "    c. 'Strength in numbers'\n",
    "    \n",
    "**ANS:** a. Wisdom of crowds\n",
    "\n",
    "**Explaination:** Refer to the concept if you got this incorrect\n",
    "\n",
    "\n",
    "2. Random forest is an example of stacking method\n",
    "\n",
    "    a. True\n",
    "    \n",
    "    b. False\n",
    "    \n",
    "    \n",
    "**ANS:** b\n",
    "\n",
    "**Explaination:** Random forest is a type of bagging method with the additional step of subsetting of features along with bootstrapping\n",
    "\n",
    "3. Bagging results in\n",
    "\n",
    "    a. Reduction of bias, increase of variance \n",
    "    \n",
    "    b. Increase of bias, reduction of variance\n",
    "    \n",
    "    c. Reduction of bias, reduction of variance\n",
    "    \n",
    "    d. Increase of bias, increase of variance\n",
    "    \n",
    "**ANS:** b\n",
    "\n",
    "**Explaination:** Bagging due to it's inherent bootstrapping almost always reduces variance but with the cost of slightly higher bias\n",
    "\n",
    "\n",
    "4. Random Forests can be used only for classification\n",
    "\n",
    "    a. True\n",
    "    \n",
    "    b. False\n",
    "    \n",
    "**ANS:** b\n",
    "\n",
    "**Explaination:** Random forest is a versatile problem solving machine learning model. It can be used for classification, regression, feature selection and clustering.\n",
    "\n",
    "\n",
    "5. Ensembling methods can always beat a single machine learning model \n",
    "    \n",
    "    a. True\n",
    "    \n",
    "    b. False\n",
    "    \n",
    "    \n",
    "**ANS:** b\n",
    "\n",
    "**Explaination:** Ensembling methods are used to combine the power of weak learners to form a strong learner. Therefore a single strong machine learning model will have comparable performance with ensemble of weak learners."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
