{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Which of the following is not an application of Language Models?\n",
    "\n",
    "A. Spell Checking \n",
    "\n",
    "B. Colorizing black and white images [Correct Answer]\n",
    "\n",
    "Explanation: Colorizing black and white images involve image processing and not an application of language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Natural languages can easily be understood by machines\n",
    "\n",
    "A. True\n",
    "\n",
    "B. False [Correct Answer]\n",
    "\n",
    "Explanation: Rules of natural languages cannot be fully specified and need context for understanding, making Natural languages hard for machines to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Which of the following assumption is the underlying principle behind N-gram models?\n",
    "\n",
    "A. Euclid Assumption\n",
    "\n",
    "B. Leibniz Assumption\n",
    "\n",
    "C. Markov Assumption[Correct Answer]\n",
    "\n",
    "Explanation: Markov assumption is the assumption which states that if we can predict the probability of future states of the process without looking at every event in the past i.e next step depends only upon the present state and not on the sequence of events that preceded it.\n",
    "\n",
    "Markov assumption fits nicely with the n-gram model because of natural language's underlying property that in most of the cases, the probability of the word depends on its surrounding words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Extrinsic evaluation is an expensive form of evaluation\n",
    "\n",
    "A. True [Correct Answer]\n",
    "\n",
    "B. False\n",
    "\n",
    "Explanation: Owing to the fact that language models would have to be run for each application, running them big NLP systems end-to-end is an expensive form of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Following is the correct formula for Laplace Smoothing:\n",
    "\n",
    "$$P_{Laplace}(w_i) = \\frac{c_i + 1}{2N + 2V}$$\n",
    "\n",
    "A. False [Correct Answer]\n",
    "\n",
    "B. True\n",
    "\n",
    "Explanation: The correct formula is $$P_{Laplace}(w_i) = \\frac{c_i + 1}{N + V}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is add-k smoothing called when k=1?\n",
    "    \n",
    "A. Laplace [Correct Answer]\n",
    "\n",
    "B. Backoff\n",
    "\n",
    "C. Interpolation\n",
    "\n",
    "Explanation: Laplace smoothing is special case of add-k smoothing when k=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Which of the following smoothing techniques does use the power of different n-gram models?\n",
    "    \n",
    "A. Laplace\n",
    "\n",
    "B. Interpolation [Correct Answer]\n",
    "\n",
    "Explanation: Interpolation uses all the available models in a linear combination whereas Laplace just updates the probability of the existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Which of the following sentences would be harder for machine to understand?\n",
    "    \n",
    "A: Sam ate food\n",
    "\n",
    "B. Sam met Mark with an injured leg [Correct Answer]\n",
    "\n",
    "Explanation: The second sentence is ambiguous(Who has injured leg? Sam or Mark?) and hence harder for machine to understand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the probability of `P(<s> Mangoes are a popular fruit </s>)`, given the following bigram probabilities:\n",
    "\n",
    "P(Mangoes|`<s>`)= 0.05\n",
    "P(fruit|popular)= 0.002\n",
    "P(a|are)=0.11    \n",
    "P(`<s>`|fruit)= 0.01    \n",
    "P(are|Mangoes)= 0.045\n",
    "P(popular|fruit)= 0.0003\n",
    "P(popular|a)= 0.06\n",
    "P(are|a)=0.009 \n",
    "\n",
    "A. ≈ 2.97E-10 [Correct Answer]\n",
    "\n",
    "B. ≈ 3.6E-10\n",
    "\n",
    "Explanation: Excluding P(popular|fruit) & P(are|a)=0.009 , we need to multiply all the probabilities which results in probability approximatel equal to 2.97E-10y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the importance of N in calculating perplexity of language models?\n",
    "\n",
    "A. Helps normalize the length of sentences [Correct Answer]\n",
    "\n",
    "B. Helps give more importance to N-gram models having higher N values \n",
    "\n",
    "Explanation: N (or rather 1/N) where N is the number of words, helps normalize for the length of the probability by the number of words. This way the longer the sentence the less probable it will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How many bigrams does the sentence 'I saw a peacock' has?\n",
    "    \n",
    "A. 3\n",
    "\n",
    "B. 5 [Correct Answer]\n",
    "\n",
    "\n",
    "Explanation: Following are the 5 bigrams:\n",
    "\n",
    "(I|`<s>`), (saw|I),(a|saw),(peacock|a),(`<s>`|peacock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the perplexity given the following bigram probability values:\n",
    "\n",
    "0.02, 0.55, 0.007\n",
    "\n",
    "A. ≈379.8 [Correct Answer]\n",
    "\n",
    "B. ≈19.2\n",
    "\n",
    "Explanation: Using the formula $$\\begin{align}PP(W) &= \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_{i-1})}}\\\\\\end{align}$$ where N=2, we get 379.8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:  Which one of the following is true?\n",
    "\n",
    "A.  After smoothing, a probability distribution might not sum up to 1 anymore\n",
    "B.  The problem of Laplace smoothing is that unseen events get too much probability mass. [Correct Answer]\n",
    "\n",
    "Explanation: In NLP applications that are very sparse, Laplace’s smoothing actually gives far too much of the probability space to unseen events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Which of the following statements is false:\n",
    "\n",
    "A. The n-gram models where n>=2 estimate have sparsity problem. \n",
    "\n",
    "B. The unigram estimate will sometimes have the problem of its numerator having value equal to 0 [Correct Answer]\n",
    "\n",
    "C. Neither of them\n",
    "\n",
    "Explanation: The unigram estimate will **NEVER** have the problem of its numerator having value equal to 0 because if numerator is 0 it means that word is not in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: After applying smoothing, sentences using n-gram model can't have total probability equal to 0\n",
    "\n",
    "A. True [Correct]\n",
    "\n",
    "B. False\n",
    "\n",
    "Explanation: Smoothing is done to resolve the sparsity problem and account for unseen words. Hence no n-gram model after smoothing can give prob. equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: If a bigram model has given 7 probabilities for n words in a sentence. What is the value of n?\n",
    "\n",
    "A. 7\n",
    "\n",
    "B. 6 [Correct Answer]\n",
    "\n",
    "C. 8\n",
    "\n",
    "Explanation: Given a sentence with n words, it will have a total of n-1 bigram probabilities.\n",
    "\n",
    "For e.g.\n",
    "\n",
    "Suppose the words in a sentence are `\"A B C D E F\"`, following will be the bigram probabilities:\n",
    "\n",
    "`\n",
    "P(A|<s>), P(B|A), P(C|B), P(D|C), P(E|D), P(F|E), P(<s>|F)\n",
    "`\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
